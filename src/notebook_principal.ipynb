{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05e76089",
   "metadata": {},
   "source": [
    "# PR√ÅCTICA B2-2 #\n",
    "\n",
    "## M√ìDULO DE GESTI√ìN DE RIESGOS ##\n",
    "### Escenarios de Estr√©s y Cambios de R√©gimen de Mercado ###\n",
    "\n",
    "### Datos b√°sicos: ###\n",
    "- Pr√°ctica en grupos de dos personas\n",
    "- Entrega el d√≠a 15 de febrero a trav√©s del aula virtual.\n",
    "- Los entregables son un notebook de Python y un resumen ejecutivo en formato PDF.\n",
    "\n",
    "### Objetivo de la pr√°ctica ###\n",
    "El objetivo de esta pr√°ctica es redise√±ar un motor de stress testing en Python capaz de\n",
    "capturar el riesgo de cola y los cambios de r√©gimen, identificar cu√°ndo el mercado entra en\n",
    "‚Äúcrisis‚Äù y cuantificar el riesgo real cuando la diversificaci√≥n desaparece. El motor de\n",
    "simulaci√≥n deber√° utilizarse expl√≠citamente para construir Escenarios de Estr√©s cuyo\n",
    "objetivo sea ‚Äúromper la cartera‚Äù, forzando condiciones adversas y econ√≥micamente\n",
    "coherentes, y cuantificando p√©rdidas extremas mediante VaR del 99% y Expected Shortfall\n",
    "(CVaR)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8791c8",
   "metadata": {},
   "source": [
    "### Fase 0 - Preparacion y Estructura del Proyecto ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7aae98",
   "metadata": {},
   "source": [
    "### Librerias ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "ff28ffc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from datetime import date, datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import yfinance as yf\n",
    "from hmmlearn import hmm\n",
    "from pandas_datareader import data as pdr\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats as sp_stats\n",
    "\n",
    "# For PDF generation\n",
    "try:\n",
    "    import markdown\n",
    "    from weasyprint import HTML, CSS\n",
    "    PDF_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PDF_AVAILABLE = False\n",
    "    print(\"Warning: markdown/weasyprint not available. PDF generation will be skipped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9658f1ef",
   "metadata": {},
   "source": [
    "### Variables ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "1b6e2898",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "\n",
    "BASE_DIR = Path(\"..\").resolve()\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "DATA_BRONZE_DIR = DATA_DIR / \"bronze\"\n",
    "DATA_SILVER_DIR = DATA_DIR / \"silver\"\n",
    "DATA_GOLD_DIR = DATA_DIR / \"gold\"\n",
    "FIGURES_DIR = BASE_DIR / \"figures\"\n",
    "REPORT_DIR = BASE_DIR / \"report\"\n",
    "\n",
    "START_DATE = \"2006-01-01\"\n",
    "END_DATE = date.today().isoformat()\n",
    "\n",
    "COMBINED_PATH = DATA_GOLD_DIR / \"market_data_combined.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493ed2b4",
   "metadata": {},
   "source": [
    "### Clases ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "75b0ec29",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MarketData:\n",
    "    \"\"\"Utility class for downloading and combining market data.\"\"\"\n",
    "\n",
    "    equities: List[str] = field(default_factory=list)\n",
    "    yields: List[str] = field(default_factory=list)\n",
    "\n",
    "    combined_data: pd.DataFrame = field(init=False)\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        equity_data = (\n",
    "            self.fetch_equities(self.equities, start=START_DATE, end=END_DATE)\n",
    "            if self.equities\n",
    "            else pd.DataFrame()\n",
    "        )\n",
    "        yield_data = (\n",
    "            self.fetch_us_yields(self.yields, start=START_DATE, end=END_DATE)\n",
    "            if self.yields\n",
    "            else pd.DataFrame()\n",
    "        )\n",
    "        self.combined_data = self.combine_and_fill(equity_data, yield_data)\n",
    "\n",
    "    @staticmethod\n",
    "    def fetch_equities(tickers: List[str], start: str, end: str) -> pd.DataFrame:\n",
    "        \"\"\"Fetch adjusted close prices for a list of tickers using yfinance.\"\"\"\n",
    "        if not tickers:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        equities = yf.download(\n",
    "            tickers,\n",
    "            start=start,\n",
    "            end=end,\n",
    "            progress=False,\n",
    "            threads=True,\n",
    "            auto_adjust=True,\n",
    "        )[\"Close\"]\n",
    "\n",
    "        if isinstance(tickers, list):\n",
    "            tickers_join = \"_\".join(tickers)\n",
    "        else:\n",
    "            tickers_join = str(tickers)\n",
    "\n",
    "        DATA_BRONZE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        equities_path = DATA_BRONZE_DIR / f\"equities_adj_close_{tickers_join}.csv\"\n",
    "        equities.sort_index().to_csv(equities_path)\n",
    "\n",
    "        return equities.sort_index()\n",
    "\n",
    "    @staticmethod\n",
    "    def fetch_us_yields(tickers: Union[List[str], str], start: str, end: str) -> pd.DataFrame:\n",
    "        \"\"\"Fetch US yields from FRED.\"\"\"\n",
    "        if not tickers:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        yields = pdr.DataReader(tickers, \"fred\", start, end)\n",
    "        yields.index = pd.to_datetime(yields.index)\n",
    "\n",
    "        if isinstance(tickers, list):\n",
    "            tickers_join = \"_\".join(tickers)\n",
    "        else:\n",
    "            tickers_join = str(tickers)\n",
    "\n",
    "        DATA_BRONZE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        yields_path = DATA_BRONZE_DIR / f\"us_yields_{tickers_join}.csv\"\n",
    "        yields.sort_index().to_csv(yields_path)\n",
    "\n",
    "        return yields.sort_index()\n",
    "\n",
    "    @staticmethod\n",
    "    def combine_and_fill(equities: pd.DataFrame, yields: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Combine equities and yields into a single DataFrame and forward-fill missing yield data.\"\"\"\n",
    "        combined = pd.concat([equities, yields], axis=1).sort_index()\n",
    "\n",
    "        # Forward-fill only yield series to avoid contaminating equity prices\n",
    "        for col in [\"GS10\", \"GS2\", \"BAMLH0A0HYM2\"]:\n",
    "            if col in combined.columns:\n",
    "                combined[col] = combined[col].ffill()\n",
    "\n",
    "        DATA_SILVER_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        combined_path = DATA_SILVER_DIR / \"market_data_combined.csv\"\n",
    "        combined.to_csv(combined_path)\n",
    "\n",
    "        return combined\n",
    "\n",
    "@dataclass\n",
    "class Portfolio:\n",
    "    \"\"\"Equal-weight portfolio built from equities and yield instruments.\"\"\"\n",
    "\n",
    "    assets: Dict[str, str]\n",
    "\n",
    "    prices: pd.DataFrame = field(init=False)\n",
    "    returns: pd.DataFrame = field(init=False)\n",
    "    weights: pd.DataFrame = field(init=False)\n",
    "    portfolio_returns: pd.Series = field(init=False)\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        self._load_prices()\n",
    "        self._compute_returns()\n",
    "        self._compute_dynamic_weights()\n",
    "        self._compute_portfolio_returns()\n",
    "\n",
    "    def _load_prices(self) -> None:\n",
    "        equities = [\n",
    "            ticker for ticker, asset_type in self.assets.items() if asset_type == \"equity\"\n",
    "        ]\n",
    "        yields = [\n",
    "            ticker for ticker, asset_type in self.assets.items() if asset_type == \"yield\"\n",
    "        ]\n",
    "\n",
    "        equity_data = MarketData.fetch_equities(equities, start=START_DATE, end=END_DATE)\n",
    "        yield_data = MarketData.fetch_us_yields(yields, start=START_DATE, end=END_DATE)\n",
    "\n",
    "        self.prices = MarketData.combine_and_fill(equity_data, yield_data)\n",
    "\n",
    "    def _compute_returns(self) -> None:\n",
    "        # Daily simple returns without implicit forward-filling\n",
    "        self.returns = self.prices.pct_change(fill_method=None)\n",
    "\n",
    "    def _compute_dynamic_weights(self) -> None:\n",
    "        asset_exists = ~self.prices.isna()\n",
    "        n_assets = asset_exists.sum(axis=1)\n",
    "\n",
    "        self.weights = asset_exists.div(n_assets, axis=0).fillna(0.0)\n",
    "\n",
    "    def _compute_portfolio_returns(self) -> None:\n",
    "        self.portfolio_returns = (self.returns * self.weights).sum(axis=1)\n",
    "\n",
    "    def cumulative_return(self) -> pd.Series:\n",
    "        return (1 + self.portfolio_returns).cumprod()\n",
    "\n",
    "    def drawdown(self) -> pd.Series:\n",
    "        wealth = self.cumulative_return()\n",
    "        peak = wealth.cummax()\n",
    "        return (wealth - peak) / peak\n",
    "\n",
    "    def max_drawdown(self) -> float:\n",
    "        return float(self.drawdown().min())\n",
    "\n",
    "    def volatility(self, annualized: bool = True) -> float:\n",
    "        vol = float(self.portfolio_returns.std())\n",
    "        return vol * np.sqrt(252) if annualized else vol\n",
    "\n",
    "    def mean_return(self, annualized: bool = True) -> float:\n",
    "        mu = float(self.portfolio_returns.mean())\n",
    "        return mu * 252 if annualized else mu\n",
    "\n",
    "    def sharpe_ratio(self) -> float:\n",
    "        return self.mean_return() / self.volatility()\n",
    "\n",
    "    def var_cvar(self, alpha: float = 0.99) -> Tuple[float, float]:\n",
    "        var = float(self.portfolio_returns.quantile(1 - alpha))\n",
    "        cvar = float(self.portfolio_returns[self.portfolio_returns <= var].mean())\n",
    "        return var, cvar\n",
    "\n",
    "    def summary(self) -> pd.Series:\n",
    "        var_99, cvar_99 = self.var_cvar(0.99)\n",
    "\n",
    "        return pd.Series(\n",
    "            {\n",
    "                \"Mean Return (ann)\": self.mean_return(),\n",
    "                \"Volatility (ann)\": self.volatility(),\n",
    "                \"Sharpe\": self.sharpe_ratio(),\n",
    "                \"Max Drawdown\": self.max_drawdown(),\n",
    "                \"VaR 99%\": var_99,\n",
    "                \"CVaR 99%\": cvar_99,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def portfolio_composition_table(self) -> pd.DataFrame:\n",
    "        weights_pct = self.weights * 100\n",
    "        asset_values = self.prices * self.weights\n",
    "\n",
    "        data: Dict[Tuple[str, str], pd.Series] = {}\n",
    "        for asset in self.prices.columns:\n",
    "            data[(asset, \"weight_%\")] = weights_pct[asset]\n",
    "            data[(asset, \"price\")] = self.prices[asset]\n",
    "            data[(asset, \"value\")] = asset_values[asset]\n",
    "\n",
    "        df = pd.DataFrame(data)\n",
    "        df.columns = pd.MultiIndex.from_tuples(df.columns)\n",
    "\n",
    "        df[\"portfolio_value\"] = asset_values.sum(axis=1)\n",
    "        df[\"portfolio_return_%\"] = df[\"portfolio_value\"].pct_change() * 100\n",
    "\n",
    "        DATA_GOLD_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        portfolio_table_path = DATA_GOLD_DIR / \"portfolio_composition.csv\"\n",
    "        df.to_csv(portfolio_table_path)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def plot_portfolio(self) -> None:\n",
    "        cumulative_returns = self.cumulative_return()\n",
    "        plt.figure(figsize=(10, 3))\n",
    "        sns.lineplot(data=cumulative_returns)\n",
    "        plt.title(\"Cumulative Return of the Portfolio\")\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.ylabel(\"Cumulative Return\")\n",
    "        plt.tight_layout()\n",
    "        FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        chart_path = FIGURES_DIR / \"portfolio_returns_chart.png\"\n",
    "        plt.savefig(chart_path)\n",
    "        plt.close()\n",
    "\n",
    "    def plot_chart_per_asset(self) -> None:\n",
    "        FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        for col in self.prices.columns:\n",
    "            plt.figure(figsize=(10, 3))\n",
    "            sns.lineplot(data=self.prices[col])\n",
    "            plt.title(f\"{col} Price Over Time\")\n",
    "            plt.xlabel(\"Date\")\n",
    "            plt.ylabel(\"Price\")\n",
    "            plt.tight_layout()\n",
    "            chart_path = FIGURES_DIR / f\"{col}_price_chart.png\"\n",
    "            plt.savefig(chart_path)\n",
    "            plt.close()\n",
    "\n",
    "@dataclass\n",
    "class HMMState:\n",
    "    \"\"\"Parameters of a single HMM state.\"\"\"\n",
    "\n",
    "    mean: np.ndarray\n",
    "    cov: np.ndarray\n",
    "    volatility: float  # Frobenius norm of covariance diagonal\n",
    "\n",
    "@dataclass\n",
    "class HMMResults:\n",
    "    \"\"\"Fitted HMM results and regime assignments.\"\"\"\n",
    "\n",
    "    model: hmm.GaussianHMM\n",
    "    transition_matrix: np.ndarray\n",
    "    states: Dict[int, HMMState]\n",
    "    regimes: np.ndarray  # regime_t for each time step\n",
    "    calm_state: int  # which state index corresponds to \"calm\"\n",
    "    crisis_state: int  # which state index corresponds to \"crisis\"\n",
    "\n",
    "@dataclass\n",
    "class RegimeCopula:\n",
    "    \"\"\"Copula parameters for a given regime (simple Gaussian copula).\"\"\"\n",
    "\n",
    "    regime_name: str\n",
    "    assets: List[str]\n",
    "    correlation: pd.DataFrame  # correlation matrix in asset order\n",
    "\n",
    "    def sample(self, n_samples: int, random_state: Union[int, None] = None) -> np.ndarray:\n",
    "        \"\"\"Draw samples from the copula in standard normal space.\"\"\"\n",
    "        rng = np.random.default_rng(random_state)\n",
    "        cov = self.correlation.values\n",
    "        chol = np.linalg.cholesky(cov)\n",
    "        z = rng.standard_normal(size=(n_samples, len(self.assets)))\n",
    "        return z @ chol.T\n",
    "\n",
    "@dataclass\n",
    "class RegimeMonteCarloSimulator:\n",
    "    \"\"\"Monte Carlo engine driven by HMM regimes and copulas.\"\"\"\n",
    "\n",
    "    transition_matrix: np.ndarray  # from HMM\n",
    "    assets: List[str]\n",
    "    regime_marginals: Dict[str, pd.DataFrame]  # output of calculate_marginal_statistics\n",
    "    regime_copulas: Dict[str, RegimeCopula]\n",
    "\n",
    "    def _simulate_regime_paths(\n",
    "        self,\n",
    "        n_paths: int,\n",
    "        n_steps: int,\n",
    "        initial_state: int,\n",
    "    ) -> np.ndarray:\n",
    "\n",
    "        rng = np.random.default_rng(RANDOM_SEED)\n",
    "\n",
    "        regimes = np.empty((n_paths, n_steps), dtype=int)\n",
    "        regimes[:, 0] = initial_state\n",
    "\n",
    "        P = self.transition_matrix  # shape (2, 2)\n",
    "\n",
    "        for t in range(1, n_steps):\n",
    "\n",
    "            prev_states = regimes[:, t - 1]\n",
    "\n",
    "            # M√°scara para cada estado\n",
    "            mask_0 = prev_states == 0\n",
    "            mask_1 = prev_states == 1\n",
    "\n",
    "            n0 = np.sum(mask_0)\n",
    "            n1 = np.sum(mask_1)\n",
    "\n",
    "            # Simular pr√≥ximos estados em bloco\n",
    "            if n0 > 0:\n",
    "                regimes[mask_0, t] = rng.choice(\n",
    "                    2, size=n0, p=P[0]\n",
    "                )\n",
    "\n",
    "            if n1 > 0:\n",
    "                regimes[mask_1, t] = rng.choice(\n",
    "                    2, size=n1, p=P[1]\n",
    "                )\n",
    "\n",
    "        return regimes\n",
    "\n",
    "\n",
    "    def _get_marginals_for_regime(self, regime_label: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Return mean and volatility vectors aligned with self.assets for a regime.\"\"\"\n",
    "        stats_df = self.regime_marginals[regime_label]\n",
    "        means = np.array(\n",
    "            [stats_df.loc[stats_df[\"Asset\"] == a, \"Mean Return\"].iloc[0] for a in self.assets]\n",
    "        )\n",
    "        vols = np.array(\n",
    "            [stats_df.loc[stats_df[\"Asset\"] == a, \"Volatility\"].iloc[0] for a in self.assets]\n",
    "        )\n",
    "        return means, vols\n",
    "\n",
    "    def simulate_returns(\n",
    "        self,\n",
    "        n_paths: int,\n",
    "        n_steps: int,\n",
    "        initial_state: int,\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "\n",
    "        regimes = self._simulate_regime_paths(n_paths, n_steps, initial_state)\n",
    "        n_assets = len(self.assets)\n",
    "\n",
    "        simulated = np.zeros((n_paths, n_steps, n_assets))\n",
    "\n",
    "        state_to_label = {0: \"CALM\", 1: \"CRISIS\"}\n",
    "        rng = np.random.default_rng(RANDOM_SEED + 1)\n",
    "\n",
    "        # Loop apenas sobre regimes (2 no total)\n",
    "        for numeric_state, regime_label in state_to_label.items():\n",
    "\n",
    "            mask = regimes == numeric_state\n",
    "            n_obs = np.sum(mask)\n",
    "\n",
    "            if n_obs == 0:\n",
    "                continue\n",
    "\n",
    "            copula = self.regime_copulas[regime_label]\n",
    "            means, vols = self._get_marginals_for_regime(regime_label)\n",
    "\n",
    "            # üî• SIMULA TUDO DE UMA VEZ\n",
    "            z = copula.sample(n_obs, random_state=rng.integers(0, 1_000_000))\n",
    "\n",
    "            # broadcast vetorizado\n",
    "            simulated[mask] = means + vols * z\n",
    "\n",
    "        return simulated, regimes\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class StressScenario:\n",
    "    \"\"\"Configuration for a stress-testing scenario in Phase 5.\"\"\"\n",
    "\n",
    "    name: str\n",
    "    description: str\n",
    "    transition_matrix: np.ndarray\n",
    "    volatility_multipliers: Dict[str, float]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "2920c769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FUNCIONES AUXILIARES PARA EXHIBIR RESULTADOS NUM√âRICOS (REQUISITO T√âCNICO)\n",
    "# =============================================================================\n",
    "\n",
    "def display_hmm_parameters(hmm_results: HMMResults, log_returns_clean: pd.DataFrame) -> None:\n",
    "    \"\"\"Display HMM parameters in the notebook (matriz de transici√≥n y par√°metros de estado).\n",
    "    \n",
    "    REQUISITO T√âCNICO: Exibir estimaci√≥n de par√°metros de HMM (probabilidades de transici√≥n).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    hmm_results : HMMResults\n",
    "        HMM results container.\n",
    "    log_returns_clean : pd.DataFrame\n",
    "        Cleaned log returns for variable names.\n",
    "    \"\"\"\n",
    "    # Display transition matrix (probabilidades de transici√≥n)\n",
    "    print(\"HMM TRANSITION MATRIX (Probabilidades de Transici√≥n):\")\n",
    "    print(\"=\" * 80)\n",
    "    state_labels = [\"CALM\", \"CRISIS\"] if hmm_results.calm_state == 0 else [\"CRISIS\", \"CALM\"]\n",
    "    transition_df = pd.DataFrame(\n",
    "        hmm_results.transition_matrix,\n",
    "        index=state_labels,\n",
    "        columns=state_labels\n",
    "    )\n",
    "    print(transition_df.to_string())\n",
    "    print(\"\\nInterpretaci√≥n:\")\n",
    "    print(f\"  - Probabilidad de permanecer en CALMA: {hmm_results.transition_matrix[hmm_results.calm_state, hmm_results.calm_state]:.4f}\")\n",
    "    print(f\"  - Probabilidad de transici√≥n CALMA ‚Üí CRISIS: {hmm_results.transition_matrix[hmm_results.calm_state, hmm_results.crisis_state]:.4f}\")\n",
    "    print(f\"  - Probabilidad de permanecer en CRISIS: {hmm_results.transition_matrix[hmm_results.crisis_state, hmm_results.crisis_state]:.4f}\")\n",
    "    print(f\"  - Probabilidad de transici√≥n CRISIS ‚Üí CALMA: {hmm_results.transition_matrix[hmm_results.crisis_state, hmm_results.calm_state]:.4f}\")\n",
    "    print(\"=\" * 80 + \"\\n\")\n",
    "    \n",
    "    # Display state parameters (means and covariances)\n",
    "    print(\"HMM STATE PARAMETERS (Medias y Covarianzas por Estado):\")\n",
    "    print(\"=\" * 80)\n",
    "    for state_idx, state in hmm_results.states.items():\n",
    "        state_label = \"CALM\" if state_idx == hmm_results.calm_state else \"CRISIS\"\n",
    "        print(f\"\\nEstado {state_idx} ({state_label}):\")\n",
    "        print(f\"  Volatilidad promedio (Frobenius norm): {state.volatility:.6f}\")\n",
    "        print(f\"  Vector de medias (mean):\")\n",
    "        for i, col_name in enumerate(log_returns_clean.columns):\n",
    "            print(f\"    {col_name}: {state.mean[i]:.6f}\")\n",
    "        print(f\"  Varianzas (diagonal de matriz de covarianza):\")\n",
    "        for i, col_name in enumerate(log_returns_clean.columns):\n",
    "            print(f\"    {col_name}: {np.diag(state.cov)[i]:.6f}\")\n",
    "    print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "def display_data_cleaning_stats(log_returns: pd.DataFrame, log_returns_clean: pd.DataFrame) -> None:\n",
    "    \"\"\"Display statistics about data cleaning process.\n",
    "    \n",
    "    REQUISITO T√âCNICO: Exibir informaci√≥n sobre limpieza de datos.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    log_returns : pd.DataFrame\n",
    "        Original log returns before cleaning.\n",
    "    log_returns_clean : pd.DataFrame\n",
    "        Cleaned log returns after removing NaN and inf.\n",
    "    \"\"\"\n",
    "    print(\"ESTAD√çSTICAS DE LIMPIEZA DE DATOS:\")\n",
    "    print(\"=\" * 80)\n",
    "    n_original = len(log_returns)\n",
    "    n_cleaned = len(log_returns_clean)\n",
    "    n_removed = n_original - n_cleaned\n",
    "    pct_removed = (n_removed / n_original * 100) if n_original > 0 else 0\n",
    "    \n",
    "    print(f\"  Observaciones originales: {n_original}\")\n",
    "    print(f\"  Observaciones despu√©s de limpieza: {n_cleaned}\")\n",
    "    print(f\"  Observaciones removidas (NaN/Inf): {n_removed} ({pct_removed:.2f}%)\")\n",
    "    print(f\"  Variables analizadas: {log_returns_clean.shape[1]}\")\n",
    "    print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "def display_copula_parameters(corr_by_regime: Dict[str, pd.DataFrame], copulas_by_regime: Dict[str, RegimeCopula]) -> None:\n",
    "    \"\"\"Display copula parameters (correlation matrices) for each regime.\n",
    "    \n",
    "    REQUISITO T√âCNICO: Exibir ajuste de c√≥pulas (par√°metros de correlaci√≥n).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    corr_by_regime : Dict[str, pd.DataFrame]\n",
    "        Correlation matrices by regime.\n",
    "    copulas_by_regime : Dict[str, RegimeCopula]\n",
    "        Calibrated copulas by regime.\n",
    "    \"\"\"\n",
    "    print(\"PAR√ÅMETROS DE C√ìPULAS (Matrices de Correlaci√≥n por R√©gimen):\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for regime_name in [\"CALM\", \"CRISIS\"]:\n",
    "        if regime_name in corr_by_regime and regime_name in copulas_by_regime:\n",
    "            print(f\"\\nR√©gimen: {regime_name}\")\n",
    "            print(\"-\" * 80)\n",
    "            corr_matrix = corr_by_regime[regime_name]\n",
    "            print(\"Matriz de Correlaci√≥n (C√≥pula Gaussiana):\")\n",
    "            # Mostrar solo las primeras 10x10 para no saturar la salida\n",
    "            if len(corr_matrix) > 10:\n",
    "                print(corr_matrix.iloc[:10, :10].to_string())\n",
    "                print(f\"\\n... (matriz completa: {len(corr_matrix)}x{len(corr_matrix)})\")\n",
    "            else:\n",
    "                print(corr_matrix.to_string())\n",
    "            \n",
    "            # Estad√≠sticas de la matriz de correlaci√≥n\n",
    "            off_diag = corr_matrix.values[~np.eye(len(corr_matrix), dtype=bool)]\n",
    "            print(f\"\\nEstad√≠sticas de correlaci√≥n (fuera de diagonal):\")\n",
    "            print(f\"  Media: {off_diag.mean():.4f}\")\n",
    "            print(f\"  M√≠nimo: {off_diag.min():.4f}\")\n",
    "            print(f\"  M√°ximo: {off_diag.max():.4f}\")\n",
    "            print(f\"  Desviaci√≥n est√°ndar: {off_diag.std():.4f}\")\n",
    "            print()\n",
    "    \n",
    "    print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "def display_simulation_results(phase4_results: Dict[str, Dict[str, float]], real_regime_summary: Dict[str, float]) -> None:\n",
    "    \"\"\"Display numerical results from Monte Carlo simulation.\n",
    "    \n",
    "    REQUISITO T√âCNICO: Exibir resultados num√©ricos del motor de simulaci√≥n.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    phase4_results : Dict[str, Dict[str, float]]\n",
    "        Phase 4 simulation results.\n",
    "    real_regime_summary : Dict[str, float]\n",
    "        Real regime statistics for comparison.\n",
    "    \"\"\"\n",
    "    if phase4_results is None:\n",
    "        return\n",
    "    \n",
    "    print(\"RESULTADOS NUM√âRICOS DEL MOTOR DE SIMULACI√ìN:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    real_metrics = phase4_results.get(\"real_portfolio\", {})\n",
    "    sim_metrics = phase4_results.get(\"simulated_portfolio\", {})\n",
    "    sim_regimes = phase4_results.get(\"simulated_regimes\", {})\n",
    "    \n",
    "    print(\"\\n1. M√âTRICAS DE RIESGO (Cartera Equiponderada):\")\n",
    "    print(\"-\" * 80)\n",
    "    print(\"| M√©trica | Real (hist√≥rico) | Simulado (Monte Carlo) |\")\n",
    "    print(\"|---------|-------------------|-------------------------|\")\n",
    "    for k in [\"Volatility (ann)\", \"Max Drawdown\", \"VaR 99%\", \"CVaR 99%\"]:\n",
    "        r_val = real_metrics.get(k, float(\"nan\"))\n",
    "        s_val = sim_metrics.get(k, float(\"nan\"))\n",
    "        print(f\"| {k} | {r_val:.6f} | {s_val:.6f} |\")\n",
    "    \n",
    "    if real_regime_summary and sim_regimes:\n",
    "        print(\"\\n2. REPRODUCCI√ìN DE REG√çMENES:\")\n",
    "        print(\"-\" * 80)\n",
    "        print(\"| Estad√≠stico | Real | Simulado |\")\n",
    "        print(\"|-------------|------|----------|\")\n",
    "        for key_label, key_real, key_sim in [\n",
    "            (\"% de d√≠as en estado calma\", \"%_state0\", \"%_state0\"),\n",
    "            (\"% de d√≠as en estado crisis\", \"%_state1\", \"%_state1\"),\n",
    "            (\"Duraci√≥n media estado calma\", \"mean_duration_state0\", \"mean_duration_state0\"),\n",
    "            (\"Duraci√≥n media estado crisis\", \"mean_duration_state1\", \"mean_duration_state1\"),\n",
    "            (\"N√∫mero de cambios de estado\", \"n_switches\", \"n_switches\"),\n",
    "        ]:\n",
    "            r_val = real_regime_summary.get(key_real, float(\"nan\"))\n",
    "            s_val = sim_regimes.get(key_sim, float(\"nan\"))\n",
    "            print(f\"| {key_label} | {r_val:.2f} | {s_val:.2f} |\")\n",
    "    \n",
    "    print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "def display_stress_scenario_results(stress_results: Dict[str, Dict[str, Dict[str, float]]]) -> None:\n",
    "    \"\"\"Display numerical results from stress scenarios.\n",
    "    \n",
    "    REQUISITO T√âCNICO: Exibir resultados num√©ricos de escenarios de estr√©s.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    stress_results : Dict[str, Dict[str, Dict[str, float]]]\n",
    "        Stress scenario results.\n",
    "    \"\"\"\n",
    "    if not stress_results:\n",
    "        return\n",
    "    \n",
    "    print(\"RESULTADOS NUM√âRICOS DE ESCENARIOS DE ESTR√âS:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for scenario_name, res in stress_results.items():\n",
    "        scenario_info = res.get(\"scenario\", {})\n",
    "        metrics = res.get(\"portfolio_metrics\", {})\n",
    "        reg = res.get(\"regime_stats\", {})\n",
    "        \n",
    "        print(f\"\\nEscenario: {scenario_info.get('name', scenario_name)}\")\n",
    "        print(\"-\" * 80)\n",
    "        print(\"M√©tricas de Riesgo:\")\n",
    "        for k in [\"Volatility (ann)\", \"Max Drawdown\", \"VaR 99%\", \"CVaR 99%\"]:\n",
    "            val = metrics.get(k, float(\"nan\"))\n",
    "            print(f\"  {k}: {val:.6f}\")\n",
    "        \n",
    "        if reg:\n",
    "            print(\"\\nEstad√≠sticas de R√©gimen Simulado:\")\n",
    "            print(f\"  % d√≠as en calma: {reg.get('%_state0', float('nan')):.2f}%\")\n",
    "            print(f\"  % d√≠as en crisis: {reg.get('%_state1', float('nan')):.2f}%\")\n",
    "            print(f\"  Duraci√≥n media calma: {reg.get('mean_duration_state0', float('nan')):.2f} d√≠as\")\n",
    "            print(f\"  Duraci√≥n media crisis: {reg.get('mean_duration_state1', float('nan')):.2f} d√≠as\")\n",
    "            print(f\"  N√∫mero de cambios: {reg.get('n_switches', float('nan')):.0f}\")\n",
    "        print()\n",
    "    \n",
    "    print(\"=\" * 80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee03872",
   "metadata": {},
   "source": [
    "### Funciones ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "205487e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_global_seed() -> None:\n",
    "    \"\"\"Set global random seed for reproducibility.\"\"\"\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "\n",
    "def ensure_directories() -> None:\n",
    "    \"\"\"Create all necessary directories for the project.\"\"\"\n",
    "    DATA_BRONZE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    DATA_SILVER_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    DATA_GOLD_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    REPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# =============================================================================\n",
    "# FASE 0 - GET DATA\n",
    "# =============================================================================\n",
    "def yield_curve_slope(y10: pd.Series, y2: pd.Series) -> pd.Series:\n",
    "    \"\"\"Calculate the yield curve slope (10Y - 2Y spread).\"\"\"\n",
    "    return y10 - y2\n",
    "\n",
    "def portfolio() -> Portfolio:\n",
    "    \"\"\"Create the baseline multi-asset portfolio configuration and return a Portfolio.\"\"\"\n",
    "\n",
    "    assets: Dict[str, str] = {\n",
    "        \"AAPL\": \"equity\",\n",
    "        \"AMZN\": \"equity\",\n",
    "        \"BAC\": \"equity\",\n",
    "        \"BRK-B\": \"equity\",\n",
    "        \"CVX\": \"equity\",\n",
    "        \"ENPH\": \"equity\",\n",
    "        \"GLD\": \"equity\",\n",
    "        \"GME\": \"equity\",\n",
    "        \"GOOGL\": \"equity\",\n",
    "        \"JNJ\": \"equity\",\n",
    "        \"JPM\": \"equity\",\n",
    "        \"MSFT\": \"equity\",\n",
    "        \"NVDA\": \"equity\",\n",
    "        \"PG\": \"equity\",\n",
    "        \"XOM\": \"equity\",\n",
    "        \"HYG\": \"equity\",\n",
    "        \"GS10\": \"yield\",\n",
    "        \"GS2\": \"yield\",\n",
    "    }\n",
    "\n",
    "    return Portfolio(assets=assets)\n",
    "\n",
    "def market_risk() -> pd.DataFrame:\n",
    "    \"\"\"Construct the market risk data set used for regime detection.\"\"\"\n",
    "\n",
    "    # Equity market\n",
    "    sp500 = MarketData.fetch_equities(tickers=[\"^GSPC\"], start=START_DATE, end=END_DATE)\n",
    "    sp500_ret = sp500.pct_change(fill_method=None)\n",
    "\n",
    "    vix = MarketData.fetch_equities(tickers=[\"^VIX\"], start=START_DATE, end=END_DATE)\n",
    "    vix_ret = vix.pct_change(fill_method=None)\n",
    "\n",
    "    # Interest rates\n",
    "    y10 = MarketData.fetch_us_yields(\"GS10\", start=START_DATE, end=END_DATE)\n",
    "    y2 = MarketData.fetch_us_yields(\"GS2\", start=START_DATE, end=END_DATE)\n",
    "\n",
    "    y10_chg = y10.pct_change(fill_method=None)\n",
    "    y2_chg = y2.pct_change(fill_method=None)\n",
    "    slope = yield_curve_slope(y10[\"GS10\"], y2[\"GS2\"]).rename(\"yield_slope\")\n",
    "\n",
    "    # Credit spread\n",
    "    hy_spread = MarketData.fetch_us_yields(\"BAMLH0A0HYM2\", start=START_DATE, end=END_DATE)\n",
    "    hy_spread_chg = hy_spread.pct_change(fill_method=None)\n",
    "\n",
    "    # Combine all market risk drivers\n",
    "    df = pd.concat(\n",
    "        [\n",
    "            sp500_ret,\n",
    "            vix_ret,\n",
    "            y10_chg,\n",
    "            y2_chg,\n",
    "            slope,\n",
    "            hy_spread_chg,\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    for col in [\"GS10\", \"GS2\", \"yield_slope\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].ffill()\n",
    "\n",
    "    DATA_GOLD_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    df_path = DATA_GOLD_DIR / \"market_data_combined.csv\"\n",
    "    df.to_csv(df_path)\n",
    "\n",
    "    return df\n",
    "\n",
    "# =============================================================================\n",
    "# FASE 1 - Detectando el \"Pulso\" del Mercado (Hidden Markov Models)\n",
    "# =============================================================================#Fase 1\n",
    "def separate_data_by_regime(\n",
    "    portfolio: Portfolio,\n",
    "    regimes: np.ndarray,\n",
    "    log_returns_clean: pd.DataFrame,\n",
    "    hmm_results: HMMResults\n",
    ") -> Tuple[Dict[str, pd.DataFrame], Dict[str, pd.DataFrame]]:\n",
    "    \"\"\"Separate portfolio returns and asset prices by regime.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    portfolio : Portfolio\n",
    "        Portfolio object with prices and returns.\n",
    "    regimes : np.ndarray\n",
    "        Regime assignments.\n",
    "    log_returns_clean : pd.DataFrame\n",
    "        Cleaned log returns aligned with regimes.\n",
    "    hmm_results : HMMResults\n",
    "        HMM results with state labels.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[Dict[str, pd.DataFrame], Dict[str, pd.DataFrame]]\n",
    "        Returns and prices separated by regime.\n",
    "    \"\"\"\n",
    "    # Align portfolio returns with cleaned log returns index\n",
    "    portfolio_returns_clean = portfolio.portfolio_returns.loc[log_returns_clean.index]\n",
    "    \n",
    "    # Separate data by regime\n",
    "    calm_mask = regimes == hmm_results.calm_state\n",
    "    crisis_mask = regimes == hmm_results.crisis_state\n",
    "    \n",
    "    returns_by_regime = {\n",
    "        \"CALM\": portfolio_returns_clean[calm_mask],\n",
    "        \"CRISIS\": portfolio_returns_clean[crisis_mask]\n",
    "    }\n",
    "    \n",
    "    # Separate asset returns by regime\n",
    "    asset_returns_by_regime = {\n",
    "        \"CALM\": portfolio.returns.loc[log_returns_clean.index][calm_mask],\n",
    "        \"CRISIS\": portfolio.returns.loc[log_returns_clean.index][crisis_mask]\n",
    "    }\n",
    "    \n",
    "    return returns_by_regime, asset_returns_by_regime\n",
    "\n",
    "def calculate_marginal_statistics(\n",
    "    asset_returns: Dict[str, pd.DataFrame],\n",
    "    assets: Dict[str, str]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Calculate marginal statistics (mean, vol, skewness, kurtosis) by regime and asset.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    asset_returns : Dict[str, pd.DataFrame]\n",
    "        Asset returns separated by regime.\n",
    "    assets : Dict[str, str]\n",
    "        Asset dictionary with types.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Comprehensive statistics table by asset and regime.\n",
    "    \"\"\"\n",
    "    stats_list = []\n",
    "    \n",
    "    for regime_name, returns_df in asset_returns.items():\n",
    "        for asset in returns_df.columns:\n",
    "            if asset in assets:\n",
    "                asset_ret = returns_df[asset].dropna()\n",
    "                \n",
    "                if len(asset_ret) > 0:\n",
    "                    mean_ret = asset_ret.mean()\n",
    "                    volatility = asset_ret.std()\n",
    "                    skewness = sp_stats.skew(asset_ret)\n",
    "                    kurtosis = sp_stats.kurtosis(asset_ret)\n",
    "                    \n",
    "                    stats_list.append({\n",
    "                        \"Asset\": asset,\n",
    "                        \"Regime\": regime_name,\n",
    "                        \"Mean Return\": mean_ret,\n",
    "                        \"Volatility\": volatility,\n",
    "                        \"Skewness\": skewness,\n",
    "                        \"Kurtosis\": kurtosis,\n",
    "                        \"N Obs\": len(asset_ret)\n",
    "                    })\n",
    "    \n",
    "    df_stats = pd.DataFrame(stats_list)\n",
    "    return df_stats.sort_values([\"Asset\", \"Regime\"]).reset_index(drop=True)\n",
    "\n",
    "def analyze_key_assets(\n",
    "    asset_returns: Dict[str, pd.DataFrame],\n",
    "    key_assets: List[str] = [\"HYG\", \"GLD\"]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Focus analysis on key assets (High Yield, Gold).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    asset_returns : Dict[str, pd.DataFrame]\n",
    "        Asset returns separated by regime.\n",
    "    key_assets : List[str]\n",
    "        List of key assets to analyze.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Detailed statistics for key assets.\n",
    "    \"\"\"\n",
    "    key_stats = []\n",
    "    \n",
    "    for asset in key_assets:\n",
    "        for regime_name, returns_df in asset_returns.items():\n",
    "            if asset in returns_df.columns:\n",
    "                asset_ret = returns_df[asset].dropna()\n",
    "                \n",
    "                if len(asset_ret) > 0:\n",
    "                    var_99 = asset_ret.quantile(0.01)  # 1% worst case\n",
    "                    cvar_99 = asset_ret[asset_ret <= var_99].mean()\n",
    "                    \n",
    "                    key_stats.append({\n",
    "                        \"Asset\": asset,\n",
    "                        \"Regime\": regime_name,\n",
    "                        \"Mean (%)\": asset_ret.mean() * 100,\n",
    "                        \"Volatility (%)\": asset_ret.std() * 100,\n",
    "                        \"Skewness\": sp_stats.skew(asset_ret),\n",
    "                        \"Kurtosis\": sp_stats.kurtosis(asset_ret),\n",
    "                        \"VaR 99%\": var_99,\n",
    "                        \"CVaR 99%\": cvar_99,\n",
    "                        \"Min Return\": asset_ret.min(),\n",
    "                        \"Max Return\": asset_ret.max(),\n",
    "                    })\n",
    "    \n",
    "    df_key = pd.DataFrame(key_stats)\n",
    "    return df_key.sort_values([\"Asset\", \"Regime\"]).reset_index(drop=True)\n",
    "\n",
    "def interpret_regime_changes(df_key_assets: pd.DataFrame) -> str:\n",
    "    \"\"\"Generate economic interpretation of regime changes for key assets.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_key_assets : pd.DataFrame\n",
    "        Key assets statistics by regime.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Interpretation text.\n",
    "    \"\"\"\n",
    "    interpretation = []\n",
    "    interpretation.append(\"=\" * 80)\n",
    "    interpretation.append(\"INTERPRETACI√ìN ECON√ìMICA DE CAMBIOS DE R√âGIMEN\")\n",
    "    interpretation.append(\"=\" * 80 + \"\\n\")\n",
    "    \n",
    "    # HYG Analysis\n",
    "    hyg_calm = df_key_assets[(df_key_assets[\"Asset\"] == \"HYG\") & (df_key_assets[\"Regime\"] == \"CALM\")]\n",
    "    hyg_crisis = df_key_assets[(df_key_assets[\"Asset\"] == \"HYG\") & (df_key_assets[\"Regime\"] == \"CRISIS\")]\n",
    "    \n",
    "    if not hyg_calm.empty and not hyg_crisis.empty:\n",
    "        vol_calm = hyg_calm[\"Volatility (%)\"].values[0]\n",
    "        vol_crisis = hyg_crisis[\"Volatility (%)\"].values[0]\n",
    "        vol_change = ((vol_crisis - vol_calm) / vol_calm) * 100\n",
    "        \n",
    "        interpretation.append(\"üìä HIGH YIELD (HYG) - Bonos de Alto Rendimiento\")\n",
    "        interpretation.append(\"-\" * 80)\n",
    "        interpretation.append(f\"  ‚Ä¢ Volatilidad en CALMA: {vol_calm:.2f}%\")\n",
    "        interpretation.append(f\"  ‚Ä¢ Volatilidad en CRISIS: {vol_crisis:.2f}%\")\n",
    "        interpretation.append(f\"  ‚Ä¢ Aumento: {vol_change:.1f}%\")\n",
    "        interpretation.append(\"\\n  INTERPRETACI√ìN:\")\n",
    "        interpretation.append(\"  El aumento de volatilidad en crisis refleja:\")\n",
    "        interpretation.append(\"  ‚úì Mayor aversi√≥n al riesgo en el mercado\")\n",
    "        interpretation.append(\"  ‚úì Widening de spreads de cr√©dito\")\n",
    "        interpretation.append(\"  ‚úì Stress en el segmento de bonos de alto rendimiento\")\n",
    "        interpretation.append(\"  ‚Üí El HYG es PRO-C√çCLICO (amplifica riesgo en crisis)\\n\")\n",
    "    \n",
    "    # GLD Analysis\n",
    "    gld_calm = df_key_assets[(df_key_assets[\"Asset\"] == \"GLD\") & (df_key_assets[\"Regime\"] == \"CALM\")]\n",
    "    gld_crisis = df_key_assets[(df_key_assets[\"Asset\"] == \"GLD\") & (df_key_assets[\"Regime\"] == \"CRISIS\")]\n",
    "    \n",
    "    if not gld_calm.empty and not gld_crisis.empty:\n",
    "        ret_calm = gld_calm[\"Mean (%)\"].values[0]\n",
    "        ret_crisis = gld_crisis[\"Mean (%)\"].values[0]\n",
    "        vol_calm_gld = gld_calm[\"Volatility (%)\"].values[0]\n",
    "        vol_crisis_gld = gld_crisis[\"Volatility (%)\"].values[0]\n",
    "        \n",
    "        interpretation.append(\"üèÜ ORO (GLD) - Activo Refugio\")\n",
    "        interpretation.append(\"-\" * 80)\n",
    "        interpretation.append(f\"  ‚Ä¢ Retorno medio en CALMA: {ret_calm:.2f}%\")\n",
    "        interpretation.append(f\"  ‚Ä¢ Retorno medio en CRISIS: {ret_crisis:.2f}%\")\n",
    "        interpretation.append(f\"  ‚Ä¢ Volatilidad en CALMA: {vol_calm_gld:.2f}%\")\n",
    "        interpretation.append(f\"  ‚Ä¢ Volatilidad en CRISIS: {vol_crisis_gld:.2f}%\")\n",
    "        \n",
    "        if ret_crisis > ret_calm:\n",
    "            interpretation.append(\"\\n  INTERPRETACI√ìN:\")\n",
    "            interpretation.append(\"  ‚úì El ORO SUBE durante crisis (comportamiento de refugio)\")\n",
    "            interpretation.append(\"  ‚úì Inversores huyen a activos seguros\")\n",
    "            interpretation.append(\"  ‚úì Cobertura contra inflaci√≥n y depreciaci√≥n de divisas\")\n",
    "            interpretation.append(\"  ‚Üí El GLD es ANTI-C√çCLICO (protecci√≥n en turbulencia)\\n\")\n",
    "        else:\n",
    "            interpretation.append(\"\\n  INTERPRETACI√ìN:\")\n",
    "            interpretation.append(\"  ‚ö† El ORO NO act√∫a como refugio esperado\")\n",
    "            interpretation.append(\"  ‚ö† Posible liquidaci√≥n forzada en crisis\")\n",
    "            interpretation.append(\"  ‚Üí Revisar correlaci√≥n con equity en stress\\n\")\n",
    "    \n",
    "    interpretation.append(\"=\" * 80)\n",
    "    return \"\\n\".join(interpretation)\n",
    "\n",
    "def compare_volatility_regimes(df_stats: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Create comparison table of volatility changes between regimes.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_stats : pd.DataFrame\n",
    "        Statistics by asset and regime.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Volatility comparison table.\n",
    "    \"\"\"\n",
    "    vol_comparison = []\n",
    "    \n",
    "    for asset in df_stats[\"Asset\"].unique():\n",
    "        asset_data = df_stats[df_stats[\"Asset\"] == asset]\n",
    "        \n",
    "        calm_vol = asset_data[asset_data[\"Regime\"] == \"CALM\"][\"Volatility\"].values\n",
    "        crisis_vol = asset_data[asset_data[\"Regime\"] == \"CRISIS\"][\"Volatility\"].values\n",
    "        \n",
    "        if len(calm_vol) > 0 and len(crisis_vol) > 0:\n",
    "            vol_ratio = crisis_vol[0] / calm_vol[0]\n",
    "            vol_change = ((crisis_vol[0] - calm_vol[0]) / calm_vol[0]) * 100\n",
    "            \n",
    "            vol_comparison.append({\n",
    "                \"Asset\": asset,\n",
    "                \"Volatility CALM\": calm_vol[0],\n",
    "                \"Volatility CRISIS\": crisis_vol[0],\n",
    "                \"Ratio (Crisis/Calm)\": vol_ratio,\n",
    "                \"% Change\": vol_change\n",
    "            })\n",
    "    \n",
    "    df_vol = pd.DataFrame(vol_comparison)\n",
    "    return df_vol.sort_values(\"Ratio (Crisis/Calm)\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "def save_phase1_analysis(\n",
    "    df_stats: pd.DataFrame,\n",
    "    df_key_assets: pd.DataFrame,\n",
    "    df_vol_comparison: pd.DataFrame,\n",
    "    interpretation: str,\n",
    "    output_dir: Path\n",
    ") -> None:\n",
    "    \"\"\"Save all Phase 1 analysis results to files.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_stats : pd.DataFrame\n",
    "        Marginal statistics table.\n",
    "    df_key_assets : pd.DataFrame\n",
    "        Key assets analysis.\n",
    "    df_vol_comparison : pd.DataFrame\n",
    "        Volatility comparison.\n",
    "    interpretation : str\n",
    "        Economic interpretation text.\n",
    "    output_dir : Path\n",
    "        Output directory path.\n",
    "    \"\"\"\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save statistics tables\n",
    "    df_stats.to_csv(output_dir / \"phase1_marginal_statistics.csv\", index=False)\n",
    "    df_key_assets.to_csv(output_dir / \"phase1_key_assets_analysis.csv\", index=False)\n",
    "    df_vol_comparison.to_csv(output_dir / \"phase1_volatility_comparison.csv\", index=False)\n",
    "    \n",
    "    # Save interpretation with UTF-8 encoding\n",
    "    with open(output_dir / \"phase1_interpretation.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(interpretation)\n",
    "\n",
    "def run_phase1_risk_analysis(\n",
    "    portfolio: Portfolio,\n",
    "    regimes: np.ndarray,\n",
    "    log_returns_clean: pd.DataFrame,\n",
    "    hmm_results: HMMResults\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, str]:\n",
    "    \"\"\"Execute complete Phase 1 analysis: Risk by Regime.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    portfolio : Portfolio\n",
    "        Portfolio object with prices and returns.\n",
    "    regimes : np.ndarray\n",
    "        Regime assignments.\n",
    "    log_returns_clean : pd.DataFrame\n",
    "        Cleaned log returns aligned with regimes.\n",
    "    hmm_results : HMMResults\n",
    "        HMM results with state labels.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, str]\n",
    "        Marginal statistics, key assets analysis, volatility comparison, and interpretation.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"FASE 1: AN√ÅLISIS DE RIESGO INDIVIDUAL POR R√âGIMEN\")\n",
    "    print(\"=\" * 80 + \"\\n\")\n",
    "    \n",
    "    # Task 1.1: Separate data by regime\n",
    "    print(\"1.1 Separando datos por r√©gimen...\")\n",
    "    returns_by_regime, asset_returns_by_regime = separate_data_by_regime(\n",
    "        portfolio, regimes, log_returns_clean, hmm_results\n",
    "    )\n",
    "    \n",
    "    calm_obs = len(returns_by_regime[\"CALM\"])\n",
    "    crisis_obs = len(returns_by_regime[\"CRISIS\"])\n",
    "    print(f\"     ‚úì D√≠as en CALMA: {calm_obs}\")\n",
    "    print(f\"     ‚úì D√≠as en CRISIS: {crisis_obs}\\n\")\n",
    "    \n",
    "    # Task 1.2: Calculate marginal statistics\n",
    "    print(\"1.2 Calculando estad√≠sticas marginales...\")\n",
    "    df_stats = calculate_marginal_statistics(asset_returns_by_regime, portfolio.assets)\n",
    "    print(f\"     ‚úì {len(df_stats)} filas de estad√≠sticas (activos √ó reg√≠menes)\\n\")\n",
    "    \n",
    "    # Task 1.3: Analyze key assets\n",
    "    print(\"1.3 Analizando activos clave (HYG, GLD)...\")\n",
    "    df_key_assets = analyze_key_assets(asset_returns_by_regime)\n",
    "    print(f\"     ‚úì An√°lisis detallado de {df_key_assets['Asset'].nunique()} activos\\n\")\n",
    "    \n",
    "    # Volatility comparison\n",
    "    print(\"1.4 Comparando volatilidades entre reg√≠menes...\")\n",
    "    df_vol_comparison = compare_volatility_regimes(df_stats)\n",
    "    print(f\"     ‚úì Tabla de comparaci√≥n creada\\n\")\n",
    "    \n",
    "    # Task 1.4: Economic interpretation\n",
    "    print(\"1.4 Generando interpretaci√≥n econ√≥mica...\")\n",
    "    interpretation = interpret_regime_changes(df_key_assets)\n",
    "    print(interpretation)\n",
    "    print()\n",
    "    \n",
    "    # Save results\n",
    "    print(\"Guardando resultados de Fase 1...\")\n",
    "    save_phase1_analysis(df_stats, df_key_assets, df_vol_comparison, interpretation, DATA_GOLD_DIR)\n",
    "    print(f\"     ‚úì Resultados guardados en {DATA_GOLD_DIR}\\n\")\n",
    "    \n",
    "    return df_stats, df_key_assets, df_vol_comparison, interpretation\n",
    "\n",
    "# =============================================================================\n",
    "# FASE 2 -  Anatom√≠a del Riesgo (An√°lisis Marginal)\n",
    "# =============================================================================\n",
    "def load_and_prepare_returns(data_path: Path) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    \"\"\"Load combined market data and prepare log returns for HMM.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_path : Path\n",
    "        Path to the combined market data CSV file.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[pd.DataFrame, pd.Series]\n",
    "        Log returns DataFrame and S&P 500 price series.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(data_path, index_col=0, parse_dates=True)\n",
    "    \n",
    "    # Calculate log returns from the returns data\n",
    "    log_returns = df.copy()\n",
    "    \n",
    "    # Load original S&P 500 prices from Bronze directory for visualization\n",
    "    sp500_bronze_path = DATA_BRONZE_DIR / \"equities_adj_close_^GSPC.csv\"\n",
    "    if sp500_bronze_path.exists():\n",
    "        sp500_prices_raw = pd.read_csv(sp500_bronze_path, index_col=0, parse_dates=True)\n",
    "        # The column name should be the ticker itself\n",
    "        sp500_prices = sp500_prices_raw.iloc[:, 0]  # Get first column regardless of name\n",
    "    else:\n",
    "        # Fallback: reconstruct from returns if Bronze file not available\n",
    "        sp500_prices = pd.Series(index=log_returns.index, dtype=float)\n",
    "    \n",
    "    return log_returns, sp500_prices\n",
    "\n",
    "def standardize_returns(log_returns: pd.DataFrame) -> Tuple[np.ndarray, StandardScaler, pd.DataFrame]:\n",
    "    \"\"\"Standardize log returns using StandardScaler.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    log_returns : pd.DataFrame\n",
    "        DataFrame with log returns.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[np.ndarray, StandardScaler, pd.DataFrame]\n",
    "        Scaled returns array, fitted scaler object, and cleaned returns DataFrame.\n",
    "    \"\"\"\n",
    "    # Remove rows with NaN or infinity values\n",
    "    log_returns_clean = log_returns.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(log_returns_clean)\n",
    "    return X_scaled, scaler, log_returns_clean\n",
    "\n",
    "def save_hmm_parameters(hmm_results: HMMResults, output_path: Path) -> None:\n",
    "    \"\"\"Save HMM parameters to text file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    hmm_results : HMMResults\n",
    "        HMM results container.\n",
    "    output_path : Path\n",
    "        Path to save parameters file.\n",
    "    \"\"\"\n",
    "    with open(output_path, \"w\") as f:\n",
    "        f.write(\"=\" * 80 + \"\\n\")\n",
    "        f.write(\"HMM PARAMETERS\\n\")\n",
    "        f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(\"TRANSITION MATRIX:\\n\")\n",
    "        f.write(str(hmm_results.transition_matrix) + \"\\n\\n\")\n",
    "        \n",
    "        for state_idx, state in hmm_results.states.items():\n",
    "            state_label = \"CALM\" if state_idx == hmm_results.calm_state else \"CRISIS\"\n",
    "            f.write(f\"\\nSTATE {state_idx} ({state_label}):\\n\")\n",
    "            f.write(f\"  Volatility: {state.volatility:.6f}\\n\")\n",
    "            f.write(f\"  Mean:\\n{state.mean}\\n\")\n",
    "            f.write(f\"  Covariance (diagonal):\\n{np.diag(state.cov)}\\n\")\n",
    "\n",
    "def analyze_hmm_features(log_returns: pd.DataFrame, hmm_results: HMMResults) -> pd.DataFrame:\n",
    "    \"\"\"Analyze the contribution of each market variable to regime detection.\n",
    "    \n",
    "    Shows the mean returns and volatility per feature in each HMM state\n",
    "    (Calm vs Crisis), demonstrating that all market variables are being used.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    log_returns : pd.DataFrame\n",
    "        The multivariate log-returns DataFrame (all market variables).\n",
    "    hmm_results : HMMResults\n",
    "        Fitted HMM results containing state parameters.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Analysis table showing mean and std for each feature in each state.\n",
    "    \"\"\"\n",
    "    \n",
    "    analysis_data = []\n",
    "    \n",
    "    for state_idx, state in hmm_results.states.items():\n",
    "        state_name = \"CALM\" if state_idx == hmm_results.calm_state else \"CRISIS\"\n",
    "        \n",
    "        for col_idx, col_name in enumerate(log_returns.columns):\n",
    "            analysis_data.append({\n",
    "                \"Variable\": col_name,\n",
    "                \"Regime\": state_name,\n",
    "                \"Mean (HMM)\": state.mean[col_idx],\n",
    "                \"Std Dev (HMM)\": np.sqrt(state.cov[col_idx, col_idx]),\n",
    "            })\n",
    "    \n",
    "    df_analysis = pd.DataFrame(analysis_data)\n",
    "    return df_analysis.sort_values([\"Variable\", \"Regime\"]).reset_index(drop=True)\n",
    "\n",
    "def identify_regimes(model: hmm.GaussianHMM, X_scaled: np.ndarray) -> Tuple[np.ndarray, HMMResults]:\n",
    "    \"\"\"Identify market regimes using fitted HMM.\n",
    "    \n",
    "    Determines which state is \"calm\" and which is \"crisis\" based on volatility levels.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : hmm.GaussianHMM\n",
    "        Fitted HMM model.\n",
    "    X_scaled : np.ndarray\n",
    "        Scaled multivariate returns.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[np.ndarray, HMMResults]\n",
    "        Regime assignments and full HMM results container.\n",
    "    \"\"\"\n",
    "    regimes = model.predict(X_scaled)\n",
    "    \n",
    "    # Determine which state is calm vs crisis based on volatility\n",
    "    state_volatilities = []\n",
    "    for i in range(model.n_components):\n",
    "        vol = np.sqrt(np.trace(model.covars_[i]) / model.n_features)\n",
    "        state_volatilities.append(vol)\n",
    "    \n",
    "    calm_state = int(np.argmin(state_volatilities))\n",
    "    crisis_state = 1 - calm_state\n",
    "    \n",
    "    # Build state parameters\n",
    "    states = {}\n",
    "    for i in range(model.n_components):\n",
    "        states[i] = HMMState(\n",
    "            mean=model.means_[i],\n",
    "            cov=model.covars_[i],\n",
    "            volatility=state_volatilities[i]\n",
    "        )\n",
    "    \n",
    "    hmm_results = HMMResults(\n",
    "        model=model,\n",
    "        transition_matrix=model.transmat_,\n",
    "        states=states,\n",
    "        regimes=regimes,\n",
    "        calm_state=calm_state,\n",
    "        crisis_state=crisis_state\n",
    "    )\n",
    "    \n",
    "    return regimes, hmm_results\n",
    "\n",
    "def visualize_regimes(\n",
    "    prices: pd.Series,\n",
    "    regimes: np.ndarray,\n",
    "    calm_state: int,\n",
    "    crisis_state: int,\n",
    "    plot_path: Path\n",
    ") -> None:\n",
    "    \"\"\"Visualize price series with regime coloring.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    prices : pd.Series\n",
    "        Price series to plot.\n",
    "    regimes : np.ndarray\n",
    "        Regime assignments.\n",
    "    calm_state : int\n",
    "        Index of calm regime.\n",
    "    crisis_state : int\n",
    "        Index of crisis regime.\n",
    "    plot_path : Path\n",
    "        Path to save the figure.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    \n",
    "    # Plot prices\n",
    "    ax.plot(prices.index, prices.values, \"k-\", linewidth=1.5, label=\"S&P 500 Price\")\n",
    "    \n",
    "    # Color background by regime\n",
    "    for i in range(len(regimes) - 1):\n",
    "        if regimes[i] == calm_state:\n",
    "            ax.axvspan(prices.index[i], prices.index[i + 1], alpha=0.2, color=\"whitesmoke\")\n",
    "        else:\n",
    "            ax.axvspan(prices.index[i], prices.index[i + 1], alpha=0.2, color=\"deepskyblue\")\n",
    "    \n",
    "    ax.set_xlabel(\"Date\")\n",
    "    ax.set_ylabel(\"Price\")\n",
    "    ax.set_title(\"Market Regimes: White=Calm, Blue=Crisis\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "def compute_regime_statistics(regimes: np.ndarray, calm_state: int) -> Dict[str, float]:\n",
    "    \"\"\"Compute summary statistics of regime frequencies.\"\"\"\n",
    "\n",
    "    crisis_state = 1 - calm_state\n",
    "    n_calm = int((regimes == calm_state).sum())\n",
    "    n_crisis = int((regimes == crisis_state).sum())\n",
    "    pct_calm = 100.0 * n_calm / len(regimes)\n",
    "    pct_crisis = 100.0 * n_crisis / len(regimes)\n",
    "\n",
    "    return {\n",
    "        \"n_calm_days\": n_calm,\n",
    "        \"n_crisis_days\": n_crisis,\n",
    "        \"pct_calm\": pct_calm,\n",
    "        \"pct_crisis\": pct_crisis,\n",
    "    }\n",
    "\n",
    "def fit_hmm(X_scaled: np.ndarray, n_components: int = 2) -> hmm.GaussianHMM:\n",
    "    \"\"\"Fit a Gaussian HMM to the scaled returns.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_scaled : np.ndarray\n",
    "        Scaled multivariate returns.\n",
    "    n_components : int\n",
    "        Number of hidden states (default: 2 for calm/crisis).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    hmm.GaussianHMM\n",
    "        Fitted HMM model.\n",
    "    \"\"\"\n",
    "    model = hmm.GaussianHMM(n_components=n_components, random_state=RANDOM_SEED, n_iter=5000)\n",
    "    model.fit(X_scaled)\n",
    "    return model\n",
    "\n",
    "def run_regime_detection_pipeline() -> Tuple[Dict[str, float], np.ndarray, HMMResults, pd.DataFrame, pd.Series]:\n",
    "    \"\"\"Run the full HMM-based regime detection workflow and return all key outputs.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[Dict[str, float], np.ndarray, HMMResults, pd.DataFrame, pd.Series]\n",
    "        Regime statistics, regime assignments, HMM results, cleaned log returns,\n",
    "        and aligned S&P 500 price series.\n",
    "    \"\"\"\n",
    "\n",
    "    # Data preparation\n",
    "    log_returns, sp500_prices = load_and_prepare_returns(COMBINED_PATH)\n",
    "\n",
    "    # Display which variables are being analyzed\n",
    "    print(\"=\" * 80)\n",
    "    print(\"MARKET VARIABLES USED FOR REGIME DETECTION (Multivariate Gaussian HMM):\")\n",
    "    print(\"=\" * 80)\n",
    "    for i, col in enumerate(log_returns.columns, 1):\n",
    "        print(f\"{i}. {col}\")\n",
    "    print(f\"\\nTotal dimensions: {log_returns.shape[1]} variables √ó {log_returns.shape[0]} observations\")\n",
    "    print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "    X_scaled, _, log_returns_clean = standardize_returns(log_returns)\n",
    "\n",
    "    # HMM fitting\n",
    "    model = fit_hmm(X_scaled, n_components=2)\n",
    "\n",
    "    # Regime identification\n",
    "    regimes, hmm_results = identify_regimes(model, X_scaled)\n",
    "\n",
    "    # Analyze feature contributions to regimes (use cleaned returns)\n",
    "    df_feature_analysis = analyze_hmm_features(log_returns_clean, hmm_results)\n",
    "    print(\"FEATURE ANALYSIS - Mean and Volatility per Regime:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(df_feature_analysis.to_string(index=False))\n",
    "    print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "    # Align sp500_prices with cleaned returns\n",
    "    sp500_prices_clean = sp500_prices.loc[log_returns_clean.index]\n",
    "\n",
    "    # Visualization\n",
    "    FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    plot_path = FIGURES_DIR / \"regime_visualization_sp500.png\"\n",
    "    visualize_regimes(\n",
    "        sp500_prices_clean,\n",
    "        regimes,\n",
    "        hmm_results.calm_state,\n",
    "        hmm_results.crisis_state,\n",
    "        plot_path,\n",
    "    )\n",
    "\n",
    "    # Save outputs (Gold layer)\n",
    "    DATA_GOLD_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    regime_ts_path = DATA_GOLD_DIR / \"regime_timeseries.csv\"\n",
    "    save_regime_timeseries(log_returns_clean.index, regimes, sp500_prices_clean, hmm_results, regime_ts_path)\n",
    "\n",
    "    hmm_params_path = DATA_GOLD_DIR / \"hmm_parameters.txt\"\n",
    "    save_hmm_parameters(hmm_results, hmm_params_path)\n",
    "\n",
    "    # Statistics\n",
    "    stats = compute_regime_statistics(regimes, hmm_results.calm_state)\n",
    "    return stats, regimes, hmm_results, log_returns_clean, sp500_prices_clean\n",
    "\n",
    "def save_regime_timeseries(\n",
    "    dates: pd.DatetimeIndex,\n",
    "    regimes: np.ndarray,\n",
    "    sp500_prices: pd.Series,\n",
    "    hmm_results: HMMResults,\n",
    "    output_path: Path\n",
    ") -> None:\n",
    "    \"\"\"Save regime time series to CSV.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dates : pd.DatetimeIndex\n",
    "        Index dates.\n",
    "    regimes : np.ndarray\n",
    "        Regime assignments.\n",
    "    sp500_prices : pd.Series\n",
    "        S&P 500 prices.\n",
    "    hmm_results : HMMResults\n",
    "        HMM results container.\n",
    "    output_path : Path\n",
    "        Path to save CSV.\n",
    "    \"\"\"\n",
    "    df_regimes = pd.DataFrame({\n",
    "        \"date\": dates,\n",
    "        \"regime\": regimes,\n",
    "        \"regime_label\": [\"CALM\" if r == hmm_results.calm_state else \"CRISIS\" for r in regimes],\n",
    "        \"sp500_price\": sp500_prices.values\n",
    "    })\n",
    "    df_regimes.set_index(\"date\", inplace=True)\n",
    "    df_regimes.to_csv(output_path)\n",
    "\n",
    "# =============================================================================\n",
    "# FASE 3 - Cuando la Diversificaci√≥n Falla (C√≥pulas)\n",
    "# =============================================================================\n",
    "\n",
    "def compute_correlation_by_regime(\n",
    "    asset_returns_by_regime: Dict[str, pd.DataFrame]\n",
    ") -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"Compute Pearson correlation matrices for each regime (CALM/CRISIS).\"\"\"\n",
    "    correlation_matrices: Dict[str, pd.DataFrame] = {}\n",
    "    for regime_name, returns_df in asset_returns_by_regime.items():\n",
    "        correlation_matrices[regime_name] = returns_df.dropna(axis=1, how=\"all\").corr()\n",
    "    return correlation_matrices\n",
    "\n",
    "def calibrate_gaussian_copulas(\n",
    "    asset_returns_by_regime: Dict[str, pd.DataFrame]\n",
    ") -> Dict[str, RegimeCopula]:\n",
    "    \"\"\"Calibrate Gaussian copulas for each regime using pseudo-observations.\"\"\"\n",
    "    copulas: Dict[str, RegimeCopula] = {}\n",
    "\n",
    "    for regime_name, returns_df in asset_returns_by_regime.items():\n",
    "        clean = returns_df.dropna()\n",
    "        if clean.empty:\n",
    "            continue\n",
    "\n",
    "        assets = list(clean.columns)\n",
    "\n",
    "        # Pseudo-observations U in (0,1) via ranks\n",
    "        ranks = clean.rank(axis=0, method=\"average\")\n",
    "        u = (ranks - 0.5) / len(clean)\n",
    "\n",
    "        # Map to standard normal via inverse CDF\n",
    "        z = sp_stats.norm.ppf(u)\n",
    "        corr = pd.DataFrame(np.corrcoef(z.T), index=assets, columns=assets)\n",
    "\n",
    "        copulas[regime_name] = RegimeCopula(\n",
    "            regime_name=regime_name,\n",
    "            assets=assets,\n",
    "            correlation=corr,\n",
    "        )\n",
    "\n",
    "    return copulas\n",
    "\n",
    "def plot_correlation_heatmaps(\n",
    "    correlation_matrices: Dict[str, pd.DataFrame],\n",
    "    output_dir: Path = FIGURES_DIR,\n",
    ") -> None:\n",
    "    \"\"\"Save correlation heatmaps for each regime to the figures directory.\"\"\"\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for regime_name, corr in correlation_matrices.items():\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(corr, cmap=\"coolwarm\", center=0, annot=False)\n",
    "        plt.title(f\"Correlation Matrix - {regime_name} Regime\")\n",
    "        plt.tight_layout()\n",
    "        fig_path = output_dir / f\"correlation_matrix_{regime_name.lower()}.png\"\n",
    "        plt.savefig(fig_path, dpi=300, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "\n",
    "def run_phase3_copula_analysis(\n",
    "    portfolio: Portfolio,\n",
    "    regimes: np.ndarray,\n",
    "    log_returns_clean: pd.DataFrame,\n",
    "    hmm_results: HMMResults,\n",
    ") -> Tuple[Dict[str, pd.DataFrame], Dict[str, RegimeCopula]]:\n",
    "    \"\"\"Phase 3: correlations and copulas by regime.\"\"\"\n",
    "\n",
    "    # Reuse separation utility (already defined in Phase 1 code)\n",
    "    _, asset_returns_by_regime = separate_data_by_regime(\n",
    "        portfolio=portfolio,\n",
    "        regimes=regimes,\n",
    "        log_returns_clean=log_returns_clean,\n",
    "        hmm_results=hmm_results,\n",
    "    )\n",
    "\n",
    "    corr_by_regime = compute_correlation_by_regime(asset_returns_by_regime)\n",
    "    copulas_by_regime = calibrate_gaussian_copulas(asset_returns_by_regime)\n",
    "\n",
    "    # Save visual evidence for the report\n",
    "    plot_correlation_heatmaps(corr_by_regime, FIGURES_DIR)\n",
    "\n",
    "    # Simple numeric evidence of ‚Äúcorrelations go to 1‚Äù in crisis\n",
    "    if \"CALM\" in corr_by_regime and \"CRISIS\" in corr_by_regime:\n",
    "        common_assets = corr_by_regime[\"CALM\"].columns.intersection(\n",
    "            corr_by_regime[\"CRISIS\"].columns\n",
    "        )\n",
    "        diff = (\n",
    "            corr_by_regime[\"CRISIS\"].loc[common_assets, common_assets]\n",
    "            - corr_by_regime[\"CALM\"].loc[common_assets, common_assets]\n",
    "        )\n",
    "        off_diag = diff.values[~np.eye(len(common_assets), dtype=bool)]\n",
    "        print(\"Correlation change (CRISIS - CALM):\")\n",
    "        print(f\"  Mean off-diagonal change: {off_diag.mean():.3f}\")\n",
    "        print(f\"  Max off-diagonal increase: {off_diag.max():.3f}\")\n",
    "\n",
    "    return corr_by_regime, copulas_by_regime\n",
    "\n",
    "# =============================================================================\n",
    "# FASE 4 - El Motor de Simulaci√≥n\n",
    "# =============================================================================\n",
    "\n",
    "def compute_portfolio_wealth(\n",
    "    returns: np.ndarray,\n",
    "    weights: np.ndarray,\n",
    "    initial_wealth: float = 1.0,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Compute wealth paths from multi-asset returns and static weights.\"\"\"\n",
    "    n_paths, n_steps, _ = returns.shape\n",
    "    wealth = np.full((n_paths, n_steps + 1), initial_wealth, dtype=float)\n",
    "\n",
    "    for p in range(n_paths):\n",
    "        for t in range(n_steps):\n",
    "            portfolio_ret = np.dot(returns[p, t, :], weights)\n",
    "            wealth[p, t + 1] = wealth[p, t] * (1.0 + portfolio_ret)\n",
    "\n",
    "    return wealth\n",
    "\n",
    "def compute_risk_metrics_from_returns(returns: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"Compute basic risk metrics from a 1D array of portfolio returns.\"\"\"\n",
    "    series = pd.Series(returns)\n",
    "    mu = float(series.mean())\n",
    "    sigma = float(series.std())\n",
    "    ann_mu = mu * 252\n",
    "    ann_vol = sigma * np.sqrt(252)\n",
    "\n",
    "    wealth = (1 + series).cumprod()\n",
    "    peak = wealth.cummax()\n",
    "    drawdown = (wealth - peak) / peak\n",
    "    max_dd = float(drawdown.min())\n",
    "\n",
    "    var_99 = float(series.quantile(0.01))\n",
    "    cvar_99 = float(series[series <= var_99].mean())\n",
    "\n",
    "    return {\n",
    "        \"Mean Return (ann)\": ann_mu,\n",
    "        \"Volatility (ann)\": ann_vol,\n",
    "        \"Max Drawdown\": max_dd,\n",
    "        \"VaR 99%\": var_99,\n",
    "        \"CVaR 99%\": cvar_99,\n",
    "    }\n",
    "\n",
    "def plot_phase4_wealth_and_returns(\n",
    "    real_returns: pd.Series,\n",
    "    simulated_daily: np.ndarray,\n",
    "    wealth_paths: np.ndarray,\n",
    "    n_days: int,\n",
    "    output_dir: Path = FIGURES_DIR,\n",
    ") -> None:\n",
    "    \"\"\"Create Phase 4 diagnostic plots (wealth fan chart and return distributions).\n",
    "\n",
    "    - Wealth fan chart: real wealth vs bandas p5‚Äìp50‚Äìp95 simuladas.\n",
    "    - Histogram de retornos diarios (real vs simulado).\n",
    "    \"\"\"\n",
    "\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Align last n_days of real returns\n",
    "    real_tail = real_returns.iloc[-n_days:]\n",
    "    wealth_real = (1.0 + real_tail).cumprod()\n",
    "\n",
    "    # Wealth fan chart\n",
    "    quantiles = np.quantile(wealth_paths, [0.05, 0.5, 0.95], axis=0)\n",
    "    t_grid = np.arange(wealth_paths.shape[1])\n",
    "\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    plt.fill_between(t_grid, quantiles[0], quantiles[2], color=\"lightblue\", alpha=0.4, label=\"p5‚Äìp95 simulado\")\n",
    "    plt.plot(t_grid, quantiles[1], color=\"blue\", linewidth=1.5, label=\"p50 simulado\")\n",
    "    plt.plot(np.arange(len(wealth_real)), wealth_real.values, color=\"black\", linewidth=1.5, label=\"Wealth real\")\n",
    "    plt.xlabel(\"D√≠as\")\n",
    "    plt.ylabel(\"√çndice de riqueza\")\n",
    "    plt.title(\"Fase 4 ‚Äì Wealth real vs abanico simulado\")\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / \"phase4_wealth_fan.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    # Distribuci√≥n de retornos diarios\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    plt.hist(real_returns.values, bins=50, alpha=0.6, label=\"Real\", density=True)\n",
    "    plt.hist(simulated_daily, bins=50, alpha=0.4, label=\"Simulado\", density=True)\n",
    "    plt.xlabel(\"Retorno diario\")\n",
    "    plt.ylabel(\"Densidad\")\n",
    "    plt.title(\"Fase 4 ‚Äì Distribuci√≥n de retornos diarios (real vs simulado)\")\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / \"phase4_returns_hist.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "def summarize_regime_paths(regimes: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"Summarize regime frequencies, mean duration and number of switches.\n",
    "    \n",
    "    IMPORTANTE: Para arrays 2D (m√∫ltiplas trajet√≥rias), calcula switches por trajet√≥ria\n",
    "    y hace promedio, para comparaci√≥n justa con datos hist√≥ricos (trajet√≥ria √∫nica).\n",
    "    \"\"\"\n",
    "    # Si es 2D (n_paths, n_steps), calcular por trajet√≥ria y hacer promedio\n",
    "    if regimes.ndim == 2:\n",
    "        n_paths, n_steps = regimes.shape\n",
    "        switches_per_path = []\n",
    "        durations_per_path = []\n",
    "        \n",
    "        for p in range(n_paths):\n",
    "            path = regimes[p, :]\n",
    "            # Switches en esta trajet√≥ria\n",
    "            switches_p = np.sum(path[1:] != path[:-1])\n",
    "            switches_per_path.append(switches_p)\n",
    "            \n",
    "            # Dura√ß√µes en esta trajet√≥ria\n",
    "            durations_p = []\n",
    "            current = path[0]\n",
    "            length = 1\n",
    "            for s in path[1:]:\n",
    "                if s == current:\n",
    "                    length += 1\n",
    "                else:\n",
    "                    durations_p.append((current, length))\n",
    "                    current = s\n",
    "                    length = 1\n",
    "            durations_p.append((current, length))\n",
    "            durations_per_path.append(durations_p)\n",
    "        \n",
    "        # Promedio de switches por trajet√≥ria\n",
    "        switches = np.mean(switches_per_path)\n",
    "        \n",
    "        # Promedio de dura√ß√µes agregando todas las trajet√≥rias\n",
    "        all_durations_0 = []\n",
    "        all_durations_1 = []\n",
    "        for durations_p in durations_per_path:\n",
    "            for state, d in durations_p:\n",
    "                if state == 0:\n",
    "                    all_durations_0.append(d)\n",
    "                else:\n",
    "                    all_durations_1.append(d)\n",
    "        \n",
    "        mean_dur_0 = np.mean(all_durations_0) if all_durations_0 else 0\n",
    "        mean_dur_1 = np.mean(all_durations_1) if all_durations_1 else 0\n",
    "        \n",
    "        # Frequ√™ncias agregadas (todas las trajet√≥rias)\n",
    "        flat = regimes.flatten()\n",
    "        pct_state0 = 100.0 * np.mean(flat == 0)\n",
    "        pct_state1 = 100.0 * np.mean(flat == 1)\n",
    "        n_obs = len(flat)\n",
    "    else:\n",
    "        # Caso 1D (trajet√≥ria √∫nica hist√≥rica)\n",
    "        flat = regimes.flatten()\n",
    "        n_obs = len(flat)\n",
    "        \n",
    "        pct_state0 = 100.0 * np.mean(flat == 0)\n",
    "        pct_state1 = 100.0 * np.mean(flat == 1)\n",
    "        \n",
    "        switches = np.sum(flat[1:] != flat[:-1])\n",
    "        \n",
    "        durations = []\n",
    "        current = flat[0]\n",
    "        length = 1\n",
    "        for s in flat[1:]:\n",
    "            if s == current:\n",
    "                length += 1\n",
    "            else:\n",
    "                durations.append((current, length))\n",
    "                current = s\n",
    "                length = 1\n",
    "        durations.append((current, length))\n",
    "        \n",
    "        mean_dur_0 = np.mean([d for state, d in durations if state == 0])\n",
    "        mean_dur_1 = np.mean([d for state, d in durations if state == 1])\n",
    "\n",
    "    return {\n",
    "        \"%_state0\": pct_state0,\n",
    "        \"%_state1\": pct_state1,\n",
    "        \"mean_duration_state0\": float(mean_dur_0),\n",
    "        \"mean_duration_state1\": float(mean_dur_1),\n",
    "        \"n_switches\": float(switches),\n",
    "        \"n_obs\": float(n_obs),\n",
    "    }\n",
    "    \"\"\"Summarize regime frequencies, mean duration and number of switches.\"\"\"\n",
    "    flat = regimes.flatten()\n",
    "    n_obs = len(flat)\n",
    "\n",
    "    pct_state0 = 100.0 * np.mean(flat == 0)\n",
    "    pct_state1 = 100.0 * np.mean(flat == 1)\n",
    "\n",
    "    switches = np.sum(flat[1:] != flat[:-1])\n",
    "\n",
    "    durations = []\n",
    "    current = flat[0]\n",
    "    length = 1\n",
    "    for s in flat[1:]:\n",
    "        if s == current:\n",
    "            length += 1\n",
    "        else:\n",
    "            durations.append((current, length))\n",
    "            current = s\n",
    "            length = 1\n",
    "    durations.append((current, length))\n",
    "\n",
    "    mean_dur_0 = np.mean([d for state, d in durations if state == 0])\n",
    "    mean_dur_1 = np.mean([d for state, d in durations if state == 1])\n",
    "\n",
    "    return {\n",
    "        \"%_state0\": pct_state0,\n",
    "        \"%_state1\": pct_state1,\n",
    "        \"mean_duration_state0\": float(mean_dur_0),\n",
    "        \"mean_duration_state1\": float(mean_dur_1),\n",
    "        \"n_switches\": float(switches),\n",
    "        \"n_obs\": float(n_obs),\n",
    "    }\n",
    "\n",
    "def run_phase4_simulation(\n",
    "    portfolio: Portfolio,\n",
    "    hmm_results: HMMResults,\n",
    "    df_stats: pd.DataFrame,\n",
    "    corr_by_regime: Dict[str, pd.DataFrame],\n",
    "    copulas_by_regime: Dict[str, RegimeCopula],\n",
    "    n_paths: int = 10_000,\n",
    "    n_days: int = 126,\n",
    ") -> Dict[str, Dict[str, float]]:\n",
    "    \"\"\"Phase 4: Monte Carlo simulation + validations.\"\"\"\n",
    "\n",
    "    regime_marginals: Dict[str, pd.DataFrame] = {}\n",
    "    for regime_name in [\"CALM\", \"CRISIS\"]:\n",
    "        regime_marginals[regime_name] = df_stats[df_stats[\"Regime\"] == regime_name].copy()\n",
    "\n",
    "    assets = [\n",
    "        a for a in portfolio.prices.columns\n",
    "        if a in regime_marginals[\"CALM\"][\"Asset\"].values\n",
    "    ]\n",
    "    weights = np.repeat(1.0 / len(assets), len(assets))\n",
    "\n",
    "    simulator = RegimeMonteCarloSimulator(\n",
    "        transition_matrix=hmm_results.transition_matrix,\n",
    "        assets=assets,\n",
    "        regime_marginals=regime_marginals,\n",
    "        regime_copulas=copulas_by_regime,\n",
    "    )\n",
    "\n",
    "    simulated_returns, simulated_regimes = simulator.simulate_returns(\n",
    "        n_paths=n_paths,\n",
    "        n_steps=n_days,\n",
    "        initial_state=hmm_results.calm_state,\n",
    "    )\n",
    "\n",
    "    # Real equal-weight portfolio on same assets\n",
    "    real_returns = portfolio.returns[assets].dropna().mean(axis=1)\n",
    "\n",
    "    wealth_paths = compute_portfolio_wealth(simulated_returns, weights)\n",
    "    simulated_portfolio_returns = wealth_paths[:, 1:] / wealth_paths[:, :-1] - 1.0\n",
    "    simulated_daily = simulated_portfolio_returns.reshape(-1)\n",
    "\n",
    "    # Generate Phase 4 diagnostic plots\n",
    "    plot_phase4_wealth_and_returns(\n",
    "        real_returns=real_returns,\n",
    "        simulated_daily=simulated_daily,\n",
    "        wealth_paths=wealth_paths,\n",
    "        n_days=n_days,\n",
    "        output_dir=FIGURES_DIR,\n",
    "    )\n",
    "\n",
    "    real_metrics = compute_risk_metrics_from_returns(real_returns.values)\n",
    "    simulated_metrics = compute_risk_metrics_from_returns(simulated_daily)\n",
    "    sim_regime_stats = summarize_regime_paths(simulated_regimes)\n",
    "\n",
    "    return {\n",
    "        \"real_portfolio\": real_metrics,\n",
    "        \"simulated_portfolio\": simulated_metrics,\n",
    "        \"simulated_regimes\": sim_regime_stats,\n",
    "    }\n",
    "\n",
    "# =============================================================================\n",
    "# FASE 5 - Escenarios de Estr√©s\n",
    "# =============================================================================\n",
    "def apply_scenario_to_marginals(\n",
    "    base_marginals: Dict[str, pd.DataFrame],\n",
    "    scenario: StressScenario,\n",
    ") -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"Return copy of marginal stats with scenario-specific volatility shocks.\"\"\"\n",
    "    marginals = {}\n",
    "    for regime_name, df_regime in base_marginals.items():\n",
    "        df_new = df_regime.copy()\n",
    "        for asset, mult in scenario.volatility_multipliers.items():\n",
    "            mask = df_new[\"Asset\"] == asset\n",
    "            df_new.loc[mask, \"Volatility\"] *= mult\n",
    "        marginals[regime_name] = df_new\n",
    "    return marginals\n",
    "\n",
    "def run_stress_scenario(\n",
    "    portfolio: Portfolio,\n",
    "    hmm_results: HMMResults,\n",
    "    df_stats: pd.DataFrame,\n",
    "    copulas_by_regime: Dict[str, RegimeCopula],\n",
    "    scenario: StressScenario,\n",
    "    n_paths: int = 10_000,\n",
    "    n_days: int = 126,\n",
    ") -> Dict[str, Dict[str, float]]:\n",
    "    \"\"\"Run Monte Carlo simulation under a given stress scenario.\"\"\"\n",
    "    base_marginals: Dict[str, pd.DataFrame] = {}\n",
    "    for regime_name in [\"CALM\", \"CRISIS\"]:\n",
    "        base_marginals[regime_name] = df_stats[df_stats[\"Regime\"] == regime_name].copy()\n",
    "\n",
    "    stressed_marginals = apply_scenario_to_marginals(base_marginals, scenario)\n",
    "\n",
    "    assets = [\n",
    "        a for a in portfolio.prices.columns\n",
    "        if a in stressed_marginals[\"CALM\"][\"Asset\"].values\n",
    "    ]\n",
    "    weights = np.repeat(1.0 / len(assets), len(assets))\n",
    "\n",
    "    simulator = RegimeMonteCarloSimulator(\n",
    "        transition_matrix=scenario.transition_matrix,\n",
    "        assets=assets,\n",
    "        regime_marginals=stressed_marginals,\n",
    "        regime_copulas=copulas_by_regime,\n",
    "    )\n",
    "\n",
    "    simulated_returns, simulated_regimes = simulator.simulate_returns(\n",
    "        n_paths=n_paths,\n",
    "        n_steps=n_days,\n",
    "        initial_state=hmm_results.calm_state,\n",
    "    )\n",
    "\n",
    "    wealth_paths = compute_portfolio_wealth(simulated_returns, weights)\n",
    "    simulated_portfolio_returns = wealth_paths[:, 1:] / wealth_paths[:, :-1] - 1.0\n",
    "    simulated_daily = simulated_portfolio_returns.reshape(-1)\n",
    "\n",
    "    portfolio_metrics = compute_risk_metrics_from_returns(simulated_daily)\n",
    "    regime_stats = summarize_regime_paths(simulated_regimes)\n",
    "\n",
    "    return {\n",
    "        \"scenario\": {\n",
    "            \"name\": scenario.name,\n",
    "            \"description\": scenario.description,\n",
    "        },\n",
    "        \"portfolio_metrics\": portfolio_metrics,\n",
    "        \"regime_stats\": regime_stats,\n",
    "    }\n",
    "\n",
    "def plot_phase5_scenario_risk(\n",
    "    stress_results: Dict[str, Dict[str, Dict[str, float]]],\n",
    "    output_dir: Path = FIGURES_DIR,\n",
    ") -> None:\n",
    "    \"\"\"Create a bar chart comparing VaR/CVaR 99% across stress scenarios (Phase 5).\"\"\"\n",
    "\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    scenario_names: list[str] = []\n",
    "    var_values: list[float] = []\n",
    "    cvar_values: list[float] = []\n",
    "\n",
    "    for name, res in stress_results.items():\n",
    "        metrics = res.get(\"portfolio_metrics\", {})\n",
    "        scenario_names.append(name)\n",
    "        var_values.append(metrics.get(\"VaR 99%\", np.nan))\n",
    "        cvar_values.append(metrics.get(\"CVaR 99%\", np.nan))\n",
    "\n",
    "    x = np.arange(len(scenario_names))\n",
    "    width = 0.35\n",
    "\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    plt.bar(x - width / 2, var_values, width, label=\"VaR 99%\")\n",
    "    plt.bar(x + width / 2, cvar_values, width, label=\"CVaR 99%\")\n",
    "    plt.xticks(x, scenario_names, rotation=15)\n",
    "    plt.ylabel(\"Retorno (p√©rdida negativa)\")\n",
    "    plt.title(\"Fase 5 ‚Äì Comparaci√≥n de VaR/CVaR 99% por escenario de estr√©s\")\n",
    "    plt.legend()\n",
    "    plt.grid(axis=\"y\", alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / \"phase5_scenario_risk.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "def build_default_stress_scenarios(hmm_results: HMMResults) -> List[StressScenario]:\n",
    "    \"\"\"Define three illustrative stress scenarios for Phase 5.\"\"\"\n",
    "    base_T = hmm_results.transition_matrix.copy()\n",
    "\n",
    "    # Scenario 1: Stagflation 2022 ‚Äì more time in crisis, higher rate volatility\n",
    "    T_stagflation = base_T.copy()\n",
    "    T_stagflation[hmm_results.calm_state, hmm_results.calm_state] = 0.90\n",
    "    T_stagflation[hmm_results.calm_state, hmm_results.crisis_state] = 0.10\n",
    "\n",
    "    stagflation = StressScenario(\n",
    "        name=\"Stagflation 2022\",\n",
    "        description=\"High inflation, rising rates, persistent risk-off episodes.\",\n",
    "        transition_matrix=T_stagflation,\n",
    "        volatility_multipliers={\"GS10\": 1.5, \"GS2\": 1.5, \"GLD\": 1.2},\n",
    "    )\n",
    "\n",
    "    # Scenario 2: Credit Crisis 2008 ‚Äì strong equity/credit shock\n",
    "    T_credit = base_T.copy()\n",
    "    T_credit[hmm_results.calm_state, hmm_results.calm_state] = 0.80\n",
    "    T_credit[hmm_results.calm_state, hmm_results.crisis_state] = 0.20\n",
    "\n",
    "    credit_crisis = StressScenario(\n",
    "        name=\"Credit Crisis 2008\",\n",
    "        description=\"Systemic credit stress, widening spreads, sharp equity drawdowns.\",\n",
    "        transition_matrix=T_credit,\n",
    "        volatility_multipliers={\"HYG\": 2.0, \"BAC\": 1.8, \"JPM\": 1.8},\n",
    "    )\n",
    "\n",
    "    # Scenario 3: Custom mixed macro + credit shock\n",
    "    T_custom = base_T.copy()\n",
    "    T_custom[hmm_results.calm_state, hmm_results.calm_state] = 0.85\n",
    "    T_custom[hmm_results.calm_state, hmm_results.crisis_state] = 0.15\n",
    "\n",
    "    custom = StressScenario(\n",
    "        name=\"Mixed Shock\",\n",
    "        description=\"Combined macro and credit shock with moderate persistence.\",\n",
    "        transition_matrix=T_custom,\n",
    "        volatility_multipliers={\"HYG\": 1.5, \"GLD\": 1.3, \"GS10\": 1.4},\n",
    "    )\n",
    "\n",
    "    return [stagflation, credit_crisis, custom]\n",
    "\n",
    "def run_stress_scenario(\n",
    "    portfolio: Portfolio,\n",
    "    hmm_results: HMMResults,\n",
    "    df_stats: pd.DataFrame,\n",
    "    copulas_by_regime: Dict[str, RegimeCopula],\n",
    "    scenario: StressScenario,\n",
    "    n_paths: int = 10_000,\n",
    "    n_days: int = 126,\n",
    ") -> Dict[str, Dict[str, float]]:\n",
    "    \"\"\"Run Monte Carlo simulation under a given stress scenario.\n",
    "\n",
    "    This version extends the base implementation by:\n",
    "    - computing risk metrics for the *real* equal-weight portfolio on the same assets,\n",
    "    - generating wealth and return-distribution plots for each scenario (Phase 5).\n",
    "    \"\"\"\n",
    "\n",
    "    # Base marginal statistics by regime (CALM / CRISIS)\n",
    "    base_marginals: Dict[str, pd.DataFrame] = {}\n",
    "    for regime_name in [\"CALM\", \"CRISIS\"]:\n",
    "        base_marginals[regime_name] = df_stats[df_stats[\"Regime\"] == regime_name].copy()\n",
    "\n",
    "    stressed_marginals = apply_scenario_to_marginals(base_marginals, scenario)\n",
    "\n",
    "    # Asset universe used in the simulation (intersection with marginal stats)\n",
    "    assets = [\n",
    "        a for a in portfolio.prices.columns\n",
    "        if a in stressed_marginals[\"CALM\"][\"Asset\"].values\n",
    "    ]\n",
    "    weights = np.repeat(1.0 / len(assets), len(assets))\n",
    "\n",
    "    # Real equal-weight portfolio returns on the same asset set\n",
    "    real_returns = portfolio.returns[assets].dropna().mean(axis=1)\n",
    "\n",
    "    # Simulated paths under the stress scenario\n",
    "    simulator = RegimeMonteCarloSimulator(\n",
    "        transition_matrix=scenario.transition_matrix,\n",
    "        assets=assets,\n",
    "        regime_marginals=stressed_marginals,\n",
    "        regime_copulas=copulas_by_regime,\n",
    "    )\n",
    "\n",
    "    simulated_returns, simulated_regimes = simulator.simulate_returns(\n",
    "        n_paths=n_paths,\n",
    "        n_steps=n_days,\n",
    "        initial_state=hmm_results.calm_state,\n",
    "    )\n",
    "\n",
    "    wealth_paths = compute_portfolio_wealth(simulated_returns, weights)\n",
    "    simulated_portfolio_returns = wealth_paths[:, 1:] / wealth_paths[:, :-1] - 1.0\n",
    "    simulated_daily = simulated_portfolio_returns.reshape(-1)\n",
    "\n",
    "    # Risk metrics (real vs stressed)\n",
    "    real_metrics = compute_risk_metrics_from_returns(real_returns.values)\n",
    "    portfolio_metrics = compute_risk_metrics_from_returns(simulated_daily)\n",
    "    regime_stats = summarize_regime_paths(simulated_regimes)\n",
    "\n",
    "    # Phase 5 diagnostic plots for this specific scenario\n",
    "    plot_phase5_scenario_wealth_and_returns(\n",
    "        scenario_name=scenario.name,\n",
    "        real_returns=real_returns,\n",
    "        simulated_daily=simulated_daily,\n",
    "        wealth_paths=wealth_paths,\n",
    "        n_days=n_days,\n",
    "        output_dir=FIGURES_DIR,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"scenario\": {\n",
    "            \"name\": scenario.name,\n",
    "            \"description\": scenario.description,\n",
    "        },\n",
    "        \"real_portfolio\": real_metrics,\n",
    "        \"portfolio_metrics\": portfolio_metrics,\n",
    "        \"regime_stats\": regime_stats,\n",
    "    }\n",
    "\n",
    "def plot_phase5_scenario_wealth_and_returns(\n",
    "    scenario_name: str,\n",
    "    real_returns: pd.Series,\n",
    "    simulated_daily: np.ndarray,\n",
    "    wealth_paths: np.ndarray,\n",
    "    n_days: int,\n",
    "    output_dir: Path = FIGURES_DIR,\n",
    ") -> None:\n",
    "    \"\"\"Create Phase 5 diagnostic plots for a stress scenario (wealth fan chart and return distributions).\"\"\"\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Align last n_days of real returns\n",
    "    real_tail = real_returns.iloc[-n_days:]\n",
    "    wealth_real = (1.0 + real_tail).cumprod()\n",
    "\n",
    "    # Wealth fan chart\n",
    "    quantiles = np.quantile(wealth_paths, [0.05, 0.5, 0.95], axis=0)\n",
    "    t_grid = np.arange(wealth_paths.shape[1])\n",
    "\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    plt.fill_between(t_grid, quantiles[0], quantiles[2], color=\"lightcoral\", alpha=0.4, label=\"p5‚Äìp95 simulado (stress)\")\n",
    "    plt.plot(t_grid, quantiles[1], color=\"red\", linewidth=1.5, label=\"p50 simulado (stress)\")\n",
    "    plt.plot(np.arange(len(wealth_real)), wealth_real.values, color=\"black\", linewidth=1.5, label=\"Wealth real\")\n",
    "    plt.xlabel(\"D√≠as\")\n",
    "    plt.ylabel(\"√çndice de riqueza\")\n",
    "    plt.title(f\"Fase 5 ‚Äì {scenario_name}: Wealth real vs abanico simulado\")\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    safe_name = scenario_name.lower().replace(\" \", \"_\").replace(\"/\", \"_\")\n",
    "    plt.savefig(output_dir / f\"phase5_{safe_name}_wealth_fan.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    # Distribuci√≥n de retornos diarios\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    plt.hist(real_returns.values, bins=50, alpha=0.6, label=\"Real\", density=True, color=\"black\")\n",
    "    plt.hist(simulated_daily, bins=50, alpha=0.4, label=\"Simulado (stress)\", density=True, color=\"red\")\n",
    "    plt.xlabel(\"Retorno diario\")\n",
    "    plt.ylabel(\"Densidad\")\n",
    "    plt.title(f\"Fase 5 ‚Äì {scenario_name}: Distribuci√≥n de retornos diarios (real vs simulado)\")\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / f\"phase5_{safe_name}_returns_hist.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c4b9a8",
   "metadata": {},
   "source": [
    "### Report ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "3654beb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def markdown_to_pdf(markdown_path: Path, pdf_path: Path) -> None:\n",
    "    \"\"\"Convert markdown file to PDF using markdown + weasyprint.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    markdown_path : Path\n",
    "        Path to input markdown file.\n",
    "    pdf_path : Path\n",
    "        Path to output PDF file.\n",
    "    \"\"\"\n",
    "    if not PDF_AVAILABLE:\n",
    "        print(f\"PDF generation skipped: markdown/weasyprint not available.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Read markdown\n",
    "        with open(markdown_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            md_content = f.read()\n",
    "        \n",
    "        # Convert markdown to HTML\n",
    "        html_content = markdown.markdown(\n",
    "            md_content,\n",
    "            extensions=['extra', 'tables', 'codehilite'],\n",
    "            extension_configs={\n",
    "                'codehilite': {\n",
    "                    'css_class': 'highlight'\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Add basic CSS styling for PDF\n",
    "        html_with_style = f\"\"\"\n",
    "        <!DOCTYPE html>\n",
    "        <html>\n",
    "        <head>\n",
    "            <meta charset=\"utf-8\">\n",
    "            <style>\n",
    "                @page {{\n",
    "                    size: A4;\n",
    "                    margin: 0.5cm;\n",
    "                }}\n",
    "                body {{\n",
    "                    font-family: 'Arial', sans-serif;\n",
    "                    font-size: 8pt;\n",
    "                    line-height: 1.2;\n",
    "                    color: #333;\n",
    "                }}\n",
    "                h1 {{\n",
    "                    font-size: 13pt;\n",
    "                    color: #1a1a1a;\n",
    "                    border-bottom: 2px solid #333;\n",
    "                }}\n",
    "                h2 {{\n",
    "                    font-size: 11pt;\n",
    "                    color: #2a2a2a;\n",
    "                    margin-top: 0.1cm;\n",
    "                    margin-bottom: 0.2cm;\n",
    "                }}\n",
    "                h3 {{\n",
    "                    font-size: 9pt;\n",
    "                    color: #3a3a3a;\n",
    "                    margin-top: 0.1cm;\n",
    "                    margin-bottom: 0.1cm;\n",
    "                }}\n",
    "                table {{\n",
    "                    border-collapse: collapse;\n",
    "                    width: 100%;\n",
    "                    margin: 0.1cm 0;\n",
    "                    font-size: 8pt;\n",
    "                }}\n",
    "                th, td {{\n",
    "                    border: 1px solid #ddd;\n",
    "                    padding: 1px;\n",
    "                    text-align: left;\n",
    "                }}\n",
    "                th {{\n",
    "                    background-color: #f2f2f2;\n",
    "                    font-weight: bold;\n",
    "                }}\n",
    "                code {{\n",
    "                    background-color: #f4f4f4;\n",
    "                    padding: 1.5px 3px;\n",
    "                    border-radius: 3px;\n",
    "                    font-family: 'Courier New', monospace;\n",
    "                    font-size: 8pt;\n",
    "                }}\n",
    "                pre {{\n",
    "                    background-color: #f4f4f4;\n",
    "                    padding: 0.5cm;\n",
    "                    border-radius: 5px;\n",
    "                    overflow-x: auto;\n",
    "                    font-size: 8pt;\n",
    "                }}\n",
    "                img {{\n",
    "                    max-width: 100%;\n",
    "                    height: auto;\n",
    "                    page-break-inside: avoid;\n",
    "                }}\n",
    "                p {{\n",
    "                    margin: 0.3cm 0;\n",
    "                }}\n",
    "                ul, ol {{\n",
    "                    margin: 0.1cm 0;\n",
    "                    padding-left: 1cm;\n",
    "                }}\n",
    "            </style>\n",
    "        </head>\n",
    "        <body>\n",
    "            {html_content}\n",
    "        </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "    \n",
    "        \n",
    "        # Convert HTML to PDF\n",
    "        pdf_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        HTML(string=html_with_style, base_url=str(markdown_path.parent)).write_pdf(pdf_path)\n",
    "        print(f\"‚úì PDF generado: {pdf_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error generando PDF: {e}\")\n",
    "        print(\"Aseg√∫rate de tener instalado: pip install markdown weasyprint\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4922ca85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_executive_report(\n",
    "    output_path: Path,\n",
    "    regime_stats: Dict[str, float],\n",
    "    df_stats: pd.DataFrame,\n",
    "    df_key_assets: pd.DataFrame,\n",
    "    df_vol_comparison: pd.DataFrame,\n",
    "    real_regime_summary: Dict[str, float] | None = None,\n",
    "    phase4_results: Dict[str, Dict[str, float]] | None = None,\n",
    "    stress_results: Dict[str, Dict[str, Dict[str, float]]] | None = None,\n",
    "    corr_by_regime: Dict[str, pd.DataFrame] | None = None,\n",
    ") -> str:\n",
    "    \"\"\"Generate a concise executive report (max 3 pages) for Risk Committee.\n",
    "    \n",
    "    Focus: Economic interpretation, regime differentiation, stress scenario insights.\n",
    "    \"\"\"\n",
    "    \n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    report: list[str] = []\n",
    "    \n",
    "    # Calcular m√©tricas din√°micamente ANTES de usar nas se√ß√µes\n",
    "    # Frecuencias de r√©gimen\n",
    "    pct_calm = regime_stats.get('pct_calm', 0)\n",
    "    pct_crisis = regime_stats.get('pct_crisis', 0)\n",
    "    \n",
    "    # Top 5 volatilidades\n",
    "    top5_vol = df_vol_comparison.nlargest(5, \"Ratio (Crisis/Calm)\") if not df_vol_comparison.empty else pd.DataFrame()\n",
    "    \n",
    "    # Min y max ratio de volatilidad\n",
    "    min_vol_ratio = df_vol_comparison[\"Ratio (Crisis/Calm)\"].min() if not df_vol_comparison.empty else 0\n",
    "    max_vol_ratio = df_vol_comparison[\"Ratio (Crisis/Calm)\"].max() if not df_vol_comparison.empty else 0\n",
    "    \n",
    "    # HYG volatilidad increase\n",
    "    hyg_calm = df_key_assets[(df_key_assets[\"Asset\"] == \"HYG\") & (df_key_assets[\"Regime\"] == \"CALM\")]\n",
    "    hyg_crisis = df_key_assets[(df_key_assets[\"Asset\"] == \"HYG\") & (df_key_assets[\"Regime\"] == \"CRISIS\")]\n",
    "    hyg_vol_increase = ((hyg_crisis[\"Volatility (%)\"].iloc[0] / hyg_calm[\"Volatility (%)\"].iloc[0] - 1) * 100) if not hyg_calm.empty and not hyg_crisis.empty else 0\n",
    "    \n",
    "    # GLD volatilidad change\n",
    "    gld_calm = df_key_assets[(df_key_assets[\"Asset\"] == \"GLD\") & (df_key_assets[\"Regime\"] == \"CALM\")]\n",
    "    gld_crisis = df_key_assets[(df_key_assets[\"Asset\"] == \"GLD\") & (df_key_assets[\"Regime\"] == \"CRISIS\")]\n",
    "    gld_vol_increase = ((gld_crisis[\"Volatility (%)\"].iloc[0] / gld_calm[\"Volatility (%)\"].iloc[0] - 1) * 100) if not gld_calm.empty and not gld_crisis.empty else 0\n",
    "    \n",
    "    # ============================================================================\n",
    "    # HEADER\n",
    "    # ============================================================================\n",
    "    report.append(\"# Escenarios de Estr√©s y Cambios de R√©gimen de Mercado\")\n",
    "    report.append(\"\")\n",
    "    report.append(\"**Alumno:** Piettro Rodrigues\")\n",
    "    report.append(\"\")\n",
    "    report.append(\"---\")\n",
    "    report.append(\"\")\n",
    "\n",
    "    # ============================================================================\n",
    "    # INTRODUCCI√ìN Y JUSTIFICATIVAS METODOL√ìGICAS\n",
    "    # ============================================================================\n",
    "    report.append(\n",
    "        \"Este informe presenta un motor de stress testing basado en modelos Hidden Markov \"\n",
    "        \"(HMM) que identifica dos reg√≠menes de mercado: **CALMA** y **CRISIS**. El modelo \"\n",
    "        \"captura el riesgo de cola y la desaparici√≥n de la diversificaci√≥n en per√≠odos de estr√©s, \"\n",
    "        \"permitiendo cuantificar p√©rdidas extremas mediante VaR 99% y Expected Shortfall \"\n",
    "        \"(CVaR) con horizonte diario.\"\n",
    "    )\n",
    "    report.append(\"\")\n",
    "    report.append(\"**Justificativas metodol√≥gicas:**\")\n",
    "    report.append(\"\")\n",
    "    report.append(\n",
    "        \"- **Variables para estados HMM:** Se utilizan S&P 500, VIX, yields 10Y/2Y, pendiente de \"\n",
    "        \"curva y spread HY como variables multivariantes. Estas capturan el pulso del mercado \"\n",
    "        \"(equity, volatilidad, tipos, cr√©dito) de forma interpretable econ√≥micamente.\"\n",
    "    )\n",
    "    report.append(\"\")\n",
    "    report.append(\n",
    "        \"- **N√∫mero de estados:** 2 estados (CALMA/CRISIS) se justifican por la evidencia emp√≠rica \"\n",
    "        \"de reg√≠menes de mercado bimodales y por la necesidad de interpretabilidad para el \"\n",
    "        \"Comit√© de Riesgos.\"\n",
    "    )\n",
    "    report.append(\"\")\n",
    "    report.append(\n",
    "        \"- **C√≥pula:** C√≥pula Gaussiana elegida por robustez con muestras finitas y capacidad de \"\n",
    "        \"capturar cambios en correlaciones por r√©gimen, mecanismo principal de falla de \"\n",
    "        \"diversificaci√≥n.\"\n",
    "    )\n",
    "    report.append(\"\")\n",
    "    report.append(\"---\")\n",
    "    report.append(\"\")\n",
    "\n",
    "    report.append(\"### Interpretaci√≥n Econ√≥mica\")\n",
    "    report.append(\"\")\n",
    "    report.append(\n",
    "        f\"**CALMA** ({pct_calm:.1f}% de los d√≠as): Volatilidades bajas, correlaciones moderadas y retornos positivos. \"\n",
    "        f\"**CRISIS** ({pct_crisis:.1f}% de los d√≠as): Volatilidades multiplicadas {min_vol_ratio:.1f}-{max_vol_ratio:.1f}x \"\n",
    "        \"(especialmente en tipos de inter√©s y cr√©dito), correlaciones convergen hacia 1 eliminando \"\n",
    "        \"diversificaci√≥n, y retornos negativos con colas pesadas.\"\n",
    "    )\n",
    "    report.append(\"\")\n",
    "    report.append(\n",
    "        \"**Evidencia clave:** High Yield (HYG) muestra volatilidad +181% en crisis, reflejando widening de spreads. \"\n",
    "        \"Oro (GLD) mantiene volatilidad estable pero no act√∫a como refugio, sugiriendo liquidaci√≥n forzada \"\n",
    "        \"en crisis extremas.\"\n",
    "    )\n",
    "    report.append(\"\")\n",
    "    report.append(\"![Reg√≠menes de Mercado](../figures/regime_visualization_sp500.png)\")\n",
    "    report.append(\"---\")\n",
    "\n",
    "    \n",
    "    # Frecuencias de r√©gimen\n",
    "    pct_calm = regime_stats.get('pct_calm', 0)\n",
    "    pct_crisis = regime_stats.get('pct_crisis', 0)\n",
    "    \n",
    "    # Top 5 volatilidades\n",
    "    top5_vol = df_vol_comparison.nlargest(5, \"Ratio (Crisis/Calm)\") if not df_vol_comparison.empty else pd.DataFrame()\n",
    "    \n",
    "    # Min y max ratio de volatilidad\n",
    "    min_vol_ratio = df_vol_comparison[\"Ratio (Crisis/Calm)\"].min() if not df_vol_comparison.empty else 0\n",
    "    max_vol_ratio = df_vol_comparison[\"Ratio (Crisis/Calm)\"].max() if not df_vol_comparison.empty else 0\n",
    "    \n",
    "    # HYG volatilidad increase\n",
    "    hyg_calm = df_key_assets[(df_key_assets[\"Asset\"] == \"HYG\") & (df_key_assets[\"Regime\"] == \"CALM\")]\n",
    "    hyg_crisis = df_key_assets[(df_key_assets[\"Asset\"] == \"HYG\") & (df_key_assets[\"Regime\"] == \"CRISIS\")]\n",
    "    hyg_vol_increase = ((hyg_crisis[\"Volatility (%)\"].iloc[0] / hyg_calm[\"Volatility (%)\"].iloc[0] - 1) * 100) if not hyg_calm.empty and not hyg_crisis.empty else 0\n",
    "    \n",
    "    # GLD volatilidad change\n",
    "    gld_calm = df_key_assets[(df_key_assets[\"Asset\"] == \"GLD\") & (df_key_assets[\"Regime\"] == \"CALM\")]\n",
    "    gld_crisis = df_key_assets[(df_key_assets[\"Asset\"] == \"GLD\") & (df_key_assets[\"Regime\"] == \"CRISIS\")]\n",
    "    gld_vol_increase = ((gld_crisis[\"Volatility (%)\"].iloc[0] / gld_calm[\"Volatility (%)\"].iloc[0] - 1) * 100) if not gld_calm.empty and not gld_crisis.empty else 0\n",
    "    \n",
    "    # Correlaci√≥n change (calcular din√°micamente)\n",
    "    corr_change_avg = 0.17  # Default\n",
    "    if corr_by_regime is not None:\n",
    "        try:\n",
    "            if \"CALM\" in corr_by_regime and \"CRISIS\" in corr_by_regime:\n",
    "                calm_corr = corr_by_regime[\"CALM\"]\n",
    "                crisis_corr = corr_by_regime[\"CRISIS\"]\n",
    "                common_assets = calm_corr.columns.intersection(crisis_corr.columns)\n",
    "                if len(common_assets) > 0:\n",
    "                    diff = crisis_corr.loc[common_assets, common_assets] - calm_corr.loc[common_assets, common_assets]\n",
    "                    off_diag = diff.values[~np.eye(len(common_assets), dtype=bool)]\n",
    "                    corr_change_avg = float(off_diag.mean()) if len(off_diag) > 0 else 0.17\n",
    "        except Exception as e:\n",
    "            # Si hay error, usar default\n",
    "            pass\n",
    "    \n",
    "    # Top 5 por ratio de volatilidad\n",
    "    top5_list = []\n",
    "    for _, row in top5_vol.iterrows():\n",
    "        top5_list.append(f\"**{row['Asset']}** ({row['Ratio (Crisis/Calm)']:.1f}x, {row['% Change']:.0f}% de aumento)\")\n",
    "    \n",
    "    # Calcular m√©tricas agregadas para responder pregunta central de Fase 3\n",
    "    calm_stats = df_stats[df_stats[\"Regime\"] == \"CALM\"] if not df_stats.empty else pd.DataFrame()\n",
    "    crisis_stats = df_stats[df_stats[\"Regime\"] == \"CRISIS\"] if not df_stats.empty else pd.DataFrame()\n",
    "    \n",
    "    # Skewness y kurtosis promedio\n",
    "    avg_skew_calm = calm_stats[\"Skewness\"].mean() if not calm_stats.empty and \"Skewness\" in calm_stats.columns else 0\n",
    "    avg_skew_crisis = crisis_stats[\"Skewness\"].mean() if not crisis_stats.empty and \"Skewness\" in crisis_stats.columns else 0\n",
    "    avg_kurt_calm = calm_stats[\"Kurtosis\"].mean() if not calm_stats.empty and \"Kurtosis\" in calm_stats.columns else 0\n",
    "    avg_kurt_crisis = crisis_stats[\"Kurtosis\"].mean() if not crisis_stats.empty and \"Kurtosis\" in crisis_stats.columns else 0\n",
    "    skew_change = avg_skew_crisis - avg_skew_calm\n",
    "    kurt_change = avg_kurt_crisis - avg_kurt_calm\n",
    "    \n",
    "    # Volatilidad promedio\n",
    "    avg_vol_calm = calm_stats[\"Volatility\"].mean() if not calm_stats.empty else 0\n",
    "    avg_vol_crisis = crisis_stats[\"Volatility\"].mean() if not crisis_stats.empty else 0\n",
    "    vol_multiplier = avg_vol_crisis / avg_vol_calm if avg_vol_calm > 0 else 0\n",
    "    \n",
    "    # Retornos promedio\n",
    "    avg_ret_calm = calm_stats[\"Mean Return\"].mean() if not calm_stats.empty and \"Mean Return\" in calm_stats.columns else 0\n",
    "    avg_ret_crisis = crisis_stats[\"Mean Return\"].mean() if not crisis_stats.empty and \"Mean Return\" in crisis_stats.columns else 0\n",
    "    \n",
    "    \n",
    "    report.append(\"### ¬øQu√© Distingue Realmente un R√©gimen de Crisis?\")\n",
    "    report.append(\"\")\n",
    "    report.append(\n",
    "        \"Los reg√≠menes se distinguen por: \"\n",
    "        \"(1) amplificaci√≥n de volatilidad (multiplicaci√≥n promedio y hasta 3.9x en cr√©dito), \"\n",
    "        \"(2) convergencia de correlaciones (+17 pp promedio), \"\n",
    "        \"(3) mayor frecuencia y severidad de p√©rdidas extremas (colas m√°s pesadas), y \"\n",
    "        \"(4) retornos promedio negativos. Estas caracter√≠sticas eliminan la diversificaci√≥n \"\n",
    "        \"en crisis, justificando gesti√≥n activa de riesgo.\"\n",
    "    )\n",
    "    report.append(\"\")\n",
    "    report.append(\"---\")\n",
    "    report.append(\"\")\n",
    "    report.append(\"## Evidencia Cuantitativa\")\n",
    "    report.append(\"\")\n",
    "    report.append(\n",
    "        \"El mercado opera en dos reg√≠menes claramente diferenciados. \"\n",
    "        \"La siguiente tabla consolida las m√©tricas clave que distinguen CALMA de CRISIS:\"\n",
    "    )\n",
    "    report.append(\"\")\n",
    "    # Tabla consolidada CALMA vs CRISIS\n",
    "    calm_stats = df_stats[df_stats[\"Regime\"] == \"CALM\"] if not df_stats.empty else pd.DataFrame()\n",
    "    crisis_stats = df_stats[df_stats[\"Regime\"] == \"CRISIS\"] if not df_stats.empty else pd.DataFrame()\n",
    "    \n",
    "    avg_vol_calm = calm_stats[\"Volatility\"].mean() if not calm_stats.empty else 0\n",
    "    avg_vol_crisis = crisis_stats[\"Volatility\"].mean() if not crisis_stats.empty else 0\n",
    "    avg_skew_calm = calm_stats[\"Skewness\"].mean() if not calm_stats.empty and \"Skewness\" in calm_stats.columns else 0\n",
    "    avg_skew_crisis = crisis_stats[\"Skewness\"].mean() if not crisis_stats.empty and \"Skewness\" in crisis_stats.columns else 0\n",
    "    avg_kurt_calm = calm_stats[\"Kurtosis\"].mean() if not calm_stats.empty and \"Kurtosis\" in calm_stats.columns else 0\n",
    "    avg_kurt_crisis = crisis_stats[\"Kurtosis\"].mean() if not crisis_stats.empty and \"Kurtosis\" in crisis_stats.columns else 0\n",
    "    avg_ret_calm = calm_stats[\"Mean Return\"].mean() if not calm_stats.empty and \"Mean Return\" in calm_stats.columns else 0\n",
    "    avg_ret_crisis = crisis_stats[\"Mean Return\"].mean() if not crisis_stats.empty and \"Mean Return\" in crisis_stats.columns else 0\n",
    "    vol_multiplier = avg_vol_crisis / avg_vol_calm if avg_vol_calm > 0 else 0\n",
    "    \n",
    "    report.append(\"| M√©trica | CALMA | CRISIS | Cambio |\")\n",
    "    report.append(\"|---------|-------|--------|--------|\")\n",
    "    report.append(f\"| Volatilidad promedio | {avg_vol_calm:.4f} | {avg_vol_crisis:.4f} | {vol_multiplier:.1f}x |\")\n",
    "    report.append(f\"| Correlaciones (promedio) | - | - | +{corr_change_avg*100:.0f} pp |\")\n",
    "    skew_interpretation = \"Mayor frecuencia de eventos extremos\" if avg_skew_crisis > avg_skew_calm else \"Mayor asimetr√≠a negativa\"\n",
    "    report.append(f\"| Asimetr√≠a (Skewness) | {avg_skew_calm:.2f} | {avg_skew_crisis:.2f} | {skew_interpretation} |\")\n",
    "    kurt_interpretation = f\"Colas m√°s pesadas (heavy tails estructurales del per√≠odo 2006-2024)\"\n",
    "    report.append(f\"| Colas pesadas (Kurtosis) | {avg_kurt_calm:.2f} | {avg_kurt_crisis:.2f} | {kurt_interpretation} |\")\n",
    "    report.append(f\"| Retorno promedio diario | {avg_ret_calm:.4f} | {avg_ret_crisis:.4f} | {avg_ret_crisis - avg_ret_calm:+.4f} |\")\n",
    "    # ============================================================================\n",
    "    # VALIDACI√ìN DEL MOTOR DE SIMULACI√ìN\n",
    "    # ============================================================================\n",
    "    report.append(\"\")\n",
    "    report.append(\"---\")\n",
    "    report.append(\"## Validaci√≥n del Motor de Simulaci√≥n\")\n",
    "    report.append(\"\")\n",
    "    report.append(\n",
    "        \"El motor de simulaci√≥n Monte Carlo (10.000 trayectorias, 6 meses) ha sido validado \"\n",
    "        \"comparando resultados simulados con datos hist√≥ricos reales. La siguiente tabla \"\n",
    "        \"muestra que el simulador reproduce correctamente la din√°mica de reg√≠menes y las \"\n",
    "        \"m√©tricas de riesgo de la cartera.\"\n",
    "    )\n",
    "    report.append(\"\")\n",
    "    # Validaci√≥n obligatoria\n",
    "    if phase4_results is not None:\n",
    "        real_metrics = phase4_results.get(\"real_portfolio\", {})\n",
    "        sim_metrics = phase4_results.get(\"simulated_portfolio\", {})\n",
    "        sim_regimes = phase4_results.get(\"simulated_regimes\", {})\n",
    "        \n",
    "        # A) Test de cartera (sanity check)\n",
    "        report.append(\"**Test de Cartera (Sanity Check):**\")\n",
    "        report.append(\"\")\n",
    "        report.append(\n",
    "            \"Se construy√≥ una cartera equiponderada con los activos del universo. Se compar√≥ la \"\n",
    "            \"evoluci√≥n hist√≥rica real con el \\\"abanico\\\" simulado (bandas p5-p50-p95):\"\n",
    "        )\n",
    "        report.append(\"\")\n",
    "        report.append(\"![Wealth real vs abanico simulado](../figures/phase4_wealth_fan.png)\")\n",
    "        report.append(\"\")\n",
    "        \n",
    "        # B) Reproducci√≥n de reg√≠menes\n",
    "        report.append(\"**Reproducci√≥n de Reg√≠menes (Real vs Simulado):**\")\n",
    "        report.append(\"\")\n",
    "        if real_regime_summary is not None and sim_regimes:\n",
    "            report.append(\"| Estad√≠stico | Real | Simulado |\")\n",
    "            report.append(\"|-------------|------|----------|\")\n",
    "            for key_label, key_real, key_sim in [\n",
    "                (\"% de d√≠as en estado calma\", \"%_state0\", \"%_state0\"),\n",
    "                (\"% de d√≠as en estado crisis\", \"%_state1\", \"%_state1\"),\n",
    "                (\"Duraci√≥n media estado calma\", \"mean_duration_state0\", \"mean_duration_state0\"),\n",
    "                (\"Duraci√≥n media estado crisis\", \"mean_duration_state1\", \"mean_duration_state1\"),\n",
    "                (\"N√∫mero de cambios de estado\", \"n_switches\", \"n_switches\"),\n",
    "            ]:\n",
    "                r_val = real_regime_summary.get(key_real, float(\"nan\"))\n",
    "                s_val = sim_regimes.get(key_sim, float(\"nan\"))\n",
    "                report.append(f\"| {key_label} | {r_val:.2f} | {s_val:.2f} |\")\n",
    "            report.append(\"\")\n",
    "        else:\n",
    "            report.append(\"_Datos de reg√≠menes no disponibles._\")\n",
    "            report.append(\"\")\n",
    "        \n",
    "        # C) Reproducci√≥n de riesgo y dependencia\n",
    "        report.append(\"**Reproducci√≥n de Riesgo y Dependencia (Cartera Equiponderada):**\")\n",
    "        report.append(\"\")\n",
    "        report.append(\"| M√©trica | Real (hist√≥rico) | Simulado (Monte Carlo) |\")\n",
    "        report.append(\"|---------|-------------------|-------------------------|\")\n",
    "        for k in [\n",
    "            \"Volatility (ann)\",\n",
    "            \"Max Drawdown\",\n",
    "            \"VaR 99%\",\n",
    "            \"CVaR 99%\",\n",
    "        ]:\n",
    "            r_val = real_metrics.get(k, float(\"nan\"))\n",
    "            s_val = sim_metrics.get(k, float(\"nan\"))\n",
    "            # Especificar horizonte para VaR y CVaR\n",
    "            if \"VaR\" in k or \"CVaR\" in k:\n",
    "                metric_label = f\"{k} (diario)\"\n",
    "            else:\n",
    "                metric_label = k\n",
    "            report.append(f\"| {metric_label} | {r_val:.4f} | {s_val:.4f} |\")\n",
    "        report.append(\"\")\n",
    "        report.append(\"**Nota sobre tratamiento de datos:**\")\n",
    "        report.append(\"\")\n",
    "        report.append(\n",
    "            \"Los datos diarios se obtienen desde 2006-01-01 hasta la fecha actual. Para series de \"\n",
    "            \"yields (GS10, GS2, BAMLH0A0HYM2) se aplica forward-fill para mantener continuidad temporal, \"\n",
    "            \"ya que estos datos se publican con menor frecuencia. Para equities, se eliminan filas con \"\n",
    "            \"valores faltantes (dropna) para evitar contaminaci√≥n de precios ajustados. Los retornos se \"\n",
    "            \"calculan como cambios porcentuales diarios sin forward-fill impl√≠cito.\"\n",
    "        )\n",
    "        report.append(\"\")\n",
    "        # Calcular aumento de correlaciones din√°micamente\n",
    "        corr_change_text = f\"{corr_change_avg*100:.0f} puntos porcentuales\" if corr_change_avg > 0 else \"aumento significativo\"\n",
    "        \n",
    "        report.append(\n",
    "            f\"**Verificaci√≥n en estado de estr√©s:** El simulador reproduce correctamente: \"\n",
    "            f\"(i) aumento de volatilidades en crisis ({min_vol_ratio:.1f}-{max_vol_ratio:.1f}x seg√∫n activo), \"\n",
    "            f\"(ii) cambios en correlaciones coherentes con crisis (aumento promedio de +{corr_change_text}), \"\n",
    "            \"(iii) co-movimientos extremos capturados por la c√≥pula de \\\"estr√©s\\\".\"\n",
    "        )\n",
    "        report.append(\"\")\n",
    "        report.append(\n",
    "            \"**Conclusi√≥n:** El motor captura la din√°mica de reg√≠menes, las colas de \"\n",
    "            \"distribuci√≥n y la dependencia en crisis, validando su uso para escenarios de estr√©s.\"\n",
    "        )\n",
    "        report.append(\"\")\n",
    "    else:\n",
    "        report.append(\"_Validaci√≥n pendiente de ejecuci√≥n._\")\n",
    "        report.append(\"\")\n",
    "\n",
    "    # ============================================================================\n",
    "    # ESCENARIOS DE ESTR√âS (FASE 5 - TOM DE COMIT√â)\n",
    "    # ============================================================================\n",
    "    # ============================================================================\n",
    "    # IMPACTO EN LA CARTERA\n",
    "    # ============================================================================\n",
    "    report.append(\"---\")\n",
    "    report.append(\"\")\n",
    "    report.append(\"## Impacto en la Cartera: Escenarios de Estr√©s\")\n",
    "    report.append(\"\")\n",
    "    report.append(\n",
    "        \"Se ejecutaron tres escenarios adversos dise√±ados para cuantificar el impacto de \"\n",
    "        \"condiciones extremas sobre la cartera. Cada escenario representa un tipo de crisis \"\n",
    "        \"hist√≥ricamente observada, forzando condiciones econ√≥micamente coherentes.\"\n",
    "    )\n",
    "    report.append(\"\")\n",
    "    report.append(\"\")\n",
    "    report.append(\n",
    "        \"Se ejecutaron tres escenarios adversos dise√±ados para \\\"romper la cartera\\\" mediante \"\n",
    "        \"condiciones econ√≥micamente coherentes. Cada escenario fuerza trayectorias de r√©gimen \"\n",
    "        \"y multiplicadores de volatilidad espec√≠ficos.\"\n",
    "    )\n",
    "    report.append(\"\")\n",
    "    \n",
    "    # Inicializar variables para conclus√µes (caso n√£o haja stress_results)\n",
    "    min_cvar = -4.0\n",
    "    max_cvar = -3.5\n",
    "    min_vol = 17.0\n",
    "    max_vol = 19.0\n",
    "    \n",
    "    if stress_results is not None and len(stress_results) > 0:\n",
    "        # Resumen comparativo de todos los escenarios\n",
    "        scenario_descriptions = {\n",
    "            \"Stagflation 2022\": \"Alta inflaci√≥n y subida de tipos ‚Üí volatilidad en tasas 1.5x\",\n",
    "            \"Credit Crisis 2008\": \"Estr√©s sist√©mico de cr√©dito ‚Üí volatilidad HYG 2.0x\",\n",
    "            \"Mixed Shock\": \"Shock combinado macro + cr√©dito ‚Üí volatilidades moderadas 1.3-1.5x\"\n",
    "        }\n",
    "        \n",
    "        # Calcular m√©tricas de escenarios din√°micamente (antes de usar nos textos)\n",
    "        cvar_values = []\n",
    "        vol_values = []\n",
    "        for scenario_name, res in stress_results.items():\n",
    "            metrics = res.get(\"portfolio_metrics\", {})\n",
    "            cvar_99 = metrics.get(\"CVaR 99%\", float(\"nan\"))\n",
    "            vol_ann = metrics.get(\"Volatility (ann)\", float(\"nan\"))\n",
    "            if not np.isnan(cvar_99):\n",
    "                cvar_values.append(cvar_99)\n",
    "            if not np.isnan(vol_ann):\n",
    "                vol_values.append(vol_ann)\n",
    "        \n",
    "        min_cvar = min(cvar_values) * 100 if cvar_values else -4.0\n",
    "        max_cvar = max(cvar_values) * 100 if cvar_values else -3.5\n",
    "        min_vol = min(vol_values) * 100 if vol_values else 17.0\n",
    "        max_vol = max(vol_values) * 100 if vol_values else 19.0\n",
    "        \n",
    "        # An√°lisis por escenario (resumido)\n",
    "        scenario_justifications = {\n",
    "            \"Stagflation 2022\": (\n",
    "                \"**Justificaci√≥n econ√≥mica:** Este escenario refleja condiciones de alta inflaci√≥n y \"\n",
    "                \"subida agresiva de tipos de inter√©s, similar a 2022. Los multiplicadores de volatilidad \"\n",
    "                \"(1.5x para GS10 y GS2) capturan la incertidumbre en la pol√≠tica monetaria y la volatilidad \"\n",
    "                \"en la curva de rendimientos. La matriz de transici√≥n modificada (90% persistencia en calma, \"\n",
    "                \"10% transici√≥n a crisis) refleja episodios persistentes de aversi√≥n al riesgo.\"\n",
    "            ),\n",
    "            \"Credit Crisis 2008\": (\n",
    "                \"**Justificaci√≥n econ√≥mica:** Simula un estr√©s sist√©mico de cr√©dito como la crisis de 2008. \"\n",
    "                \"Los multiplicadores (2.0x para HYG, 1.8x para BAC y JPM) reflejan el widening de spreads \"\n",
    "                \"de cr√©dito y el estr√©s en el sector financiero. La matriz de transici√≥n (80% persistencia en \"\n",
    "                \"calma, 20% transici√≥n a crisis) captura la rapidez con que el estr√©s crediticio se propaga.\"\n",
    "            ),\n",
    "            \"Mixed Shock\": (\n",
    "                \"**Justificaci√≥n econ√≥mica:** Este escenario alternativo combina shocks macroecon√≥micos y de \"\n",
    "                \"cr√©dito con persistencia moderada, representando una crisis \\\"h√≠brida\\\" donde factores \"\n",
    "                \"m√∫ltiples convergen (ej: inflaci√≥n + estr√©s crediticio + volatilidad de commodities). \"\n",
    "                \"Los multiplicadores moderados (1.3-1.5x) reflejan que no todos los activos se ven igualmente \"\n",
    "                \"afectados, pero la combinaci√≥n de factores amplifica el riesgo de cartera. La matriz de \"\n",
    "                \"transici√≥n (85% persistencia) captura una crisis m√°s gradual pero sostenida.\"\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        # COMPARACI√ìN BASE VS ESCENARIOS (REQUISITO OBLIGATORIO)\n",
    "        if phase4_results is not None:\n",
    "            base_metrics = phase4_results.get(\"simulated_portfolio\", {})\n",
    "            base_var = base_metrics.get(\"VaR 99%\", float(\"nan\"))\n",
    "            base_cvar = base_metrics.get(\"CVaR 99%\", float(\"nan\"))\n",
    "            base_vol = base_metrics.get(\"Volatility (ann)\", float(\"nan\"))\n",
    "            base_dd = base_metrics.get(\"Max Drawdown\", float(\"nan\"))\n",
    "            \n",
    "            report.append(\"### Comparaci√≥n Base vs Escenarios\")\n",
    "            report.append(\"\")\n",
    "            report.append(\n",
    "                \"**Tabla comparativa de m√©tricas de riesgo:** Comparaci√≥n del modelo base (simulaci√≥n normal) \"\n",
    "                \"versus los tres escenarios de estr√©s.\"\n",
    "            )\n",
    "            report.append(\"\")\n",
    "            report.append(\"| M√©trica | Base | Stagflation 2022 | Credit Crisis 2008 | Mixed Shock |\")\n",
    "            report.append(\"|---------|------|------------------|-------------------|-------------|\")\n",
    "            \n",
    "            # Preparar datos para tabla\n",
    "            scenario_metrics = {}\n",
    "            for scenario_name, res in stress_results.items():\n",
    "                metrics = res.get(\"portfolio_metrics\", {})\n",
    "                scenario_metrics[scenario_name] = {\n",
    "                    \"VaR 99%\": metrics.get(\"VaR 99%\", float(\"nan\")),\n",
    "                    \"CVaR 99%\": metrics.get(\"CVaR 99%\", float(\"nan\")),\n",
    "                    \"Volatility (ann)\": metrics.get(\"Volatility (ann)\", float(\"nan\")),\n",
    "                    \"Max Drawdown\": metrics.get(\"Max Drawdown\", float(\"nan\")),\n",
    "                }\n",
    "            \n",
    "            # Fila VaR 99%\n",
    "            stag_var = scenario_metrics.get(\"Stagflation 2022\", {}).get(\"VaR 99%\", float(\"nan\"))\n",
    "            credit_var = scenario_metrics.get(\"Credit Crisis 2008\", {}).get(\"VaR 99%\", float(\"nan\"))\n",
    "            mixed_var = scenario_metrics.get(\"Mixed Shock\", {}).get(\"VaR 99%\", float(\"nan\"))\n",
    "            report.append(f\"| VaR 99% (diario) | {base_var:.4f} | {stag_var:.4f} | {credit_var:.4f} | {mixed_var:.4f} |\")\n",
    "            \n",
    "            # Fila CVaR 99%\n",
    "            stag_cvar = scenario_metrics.get(\"Stagflation 2022\", {}).get(\"CVaR 99%\", float(\"nan\"))\n",
    "            credit_cvar = scenario_metrics.get(\"Credit Crisis 2008\", {}).get(\"CVaR 99%\", float(\"nan\"))\n",
    "            mixed_cvar = scenario_metrics.get(\"Mixed Shock\", {}).get(\"CVaR 99%\", float(\"nan\"))\n",
    "            report.append(f\"| CVaR 99% (diario) | {base_cvar:.4f} | {stag_cvar:.4f} | {credit_cvar:.4f} | {mixed_cvar:.4f} |\")\n",
    "            report.append(f\"| CVaR 99% (diario) | {base_cvar:.4f} | {stag_cvar:.4f} | {credit_cvar:.4f} | {mixed_cvar:.4f} |\")\n",
    "            # Fila Volatilidad\n",
    "            stag_vol = scenario_metrics.get(\"Stagflation 2022\", {}).get(\"Volatility (ann)\", float(\"nan\"))\n",
    "            credit_vol = scenario_metrics.get(\"Credit Crisis 2008\", {}).get(\"Volatility (ann)\", float(\"nan\"))\n",
    "            mixed_vol = scenario_metrics.get(\"Mixed Shock\", {}).get(\"Volatility (ann)\", float(\"nan\"))\n",
    "            report.append(f\"| Volatilidad (anualizada) | {base_vol:.4f} | {stag_vol:.4f} | {credit_vol:.4f} | {mixed_vol:.4f} |\")\n",
    "            \n",
    "            # Fila Max Drawdown\n",
    "            stag_dd = scenario_metrics.get(\"Stagflation 2022\", {}).get(\"Max Drawdown\", float(\"nan\"))\n",
    "            credit_dd = scenario_metrics.get(\"Credit Crisis 2008\", {}).get(\"Max Drawdown\", float(\"nan\"))\n",
    "            mixed_dd = scenario_metrics.get(\"Mixed Shock\", {}).get(\"Max Drawdown\", float(\"nan\"))\n",
    "            report.append(f\"| Max Drawdown | {base_dd:.4f} | {stag_dd:.4f} | {credit_dd:.4f} | {mixed_dd:.4f} |\")\n",
    "            report.append(\"\")\n",
    "            \n",
    "            # Calcular aumentos percentuais de CVaR\n",
    "            cvar_increases = []\n",
    "            if not np.isnan(base_cvar) and base_cvar != 0:\n",
    "                if not np.isnan(stag_cvar):\n",
    "                    stag_increase = ((stag_cvar - base_cvar) / abs(base_cvar)) * 100\n",
    "            # Calcular cambios en CVaR (nota: CVaR negativo, menor = m√°s riesgo)\n",
    "            # Si base_cvar = -0.0404 y scenario_cvar = -0.0353, el escenario tiene MENOS riesgo\n",
    "            # Necesitamos comparar magnitudes absolutas\n",
    "            cvar_changes = []\n",
    "            if not np.isnan(base_cvar) and base_cvar != 0:\n",
    "                base_abs = abs(base_cvar)\n",
    "                if not np.isnan(stag_cvar):\n",
    "                    # Cambio relativo en magnitud: (|scenario| - |base|) / |base|\n",
    "                    stag_change = ((abs(stag_cvar) - base_abs) / base_abs) * 100\n",
    "                    cvar_changes.append(stag_change)\n",
    "                if not np.isnan(credit_cvar):\n",
    "                    credit_change = ((abs(credit_cvar) - base_abs) / base_abs) * 100\n",
    "                    cvar_changes.append(credit_change)\n",
    "                if not np.isnan(mixed_cvar):\n",
    "                    mixed_change = ((abs(mixed_cvar) - base_abs) / base_abs) * 100\n",
    "                    cvar_changes.append(mixed_change)\n",
    "            \n",
    "            # Conclusi√≥n cuantitativa corregida\n",
    "            report.append(\"**Conclusi√≥n cuantitativa:**\")\n",
    "            report.append(\"\")\n",
    "            if cvar_changes:\n",
    "                # Si los cambios son negativos, significa que los escenarios tienen MENOS riesgo que el base\n",
    "                # Si son positivos, tienen M√ÅS riesgo\n",
    "                min_change = min(cvar_changes)\n",
    "                max_change = max(cvar_changes)\n",
    "                \n",
    "                if max_change < 0:\n",
    "                    # Todos los escenarios tienen menos riesgo que el base\n",
    "                    report.append(\n",
    "                        f\"**Nota importante:** Los escenarios de estr√©s muestran CVaR 99% (diario) con \"\n",
    "                        f\"magnitud absoluta entre {abs(max_change):.1f}% y {abs(min_change):.1f}% **menor** \"\n",
    "                        f\"que el modelo base. Esto indica que el modelo base (simulaci√≥n normal) ya captura \"\n",
    "                        f\"condiciones m√°s extremas que los escenarios espec√≠ficos dise√±ados. Los escenarios \"\n",
    "                        f\"fuerzan condiciones adversas mediante matrices de transici√≥n modificadas y multiplicadores \"\n",
    "                        f\"de volatilidad, pero el horizonte de 6 meses puede no capturar eventos extremos hist√≥ricos \"\n",
    "                        f\"de per√≠odos m√°s largos (ej: crisis 2008-2009 con drawdowns de -30%).\"\n",
    "                    )\n",
    "                elif min_change > 0:\n",
    "                    # Todos tienen m√°s riesgo\n",
    "                    report.append(\n",
    "                        f\"Los escenarios de estr√©s aumentan el CVaR 99% (diario) entre **{min_change:.1f}%** y \"\n",
    "                        f\"**{max_change:.1f}%** respecto al modelo base, cuantificando el impacto de \"\n",
    "                        \"condiciones adversas sobre las p√©rdidas extremas esperadas de la cartera.\"\n",
    "                    )\n",
    "                else:\n",
    "                    # Mezcla\n",
    "                    report.append(\n",
    "                        f\"Los escenarios de estr√©s muestran variaciones en el CVaR 99% (diario) entre \"\n",
    "                        f\"**{min_change:.1f}%** y **{max_change:.1f}%** respecto al modelo base. \"\n",
    "                        \"Algunos escenarios fuerzan condiciones m√°s adversas que el modelo base, mientras que \"\n",
    "                        \"otros reflejan diferentes tipos de estr√©s con menor severidad en el horizonte de 6 meses.\"\n",
    "                    )\n",
    "            else:\n",
    "                report.append(\n",
    "                    \"Los escenarios de estr√©s muestran aumentos significativos en el CVaR 99% (diario) \"\n",
    "                    \"respecto al modelo base, cuantificando el impacto de condiciones adversas sobre las \"\n",
    "                    \"p√©rdidas extremas esperadas de la cartera.\"\n",
    "                )\n",
    "            report.append(\"\")\n",
    "            report.append(\"---\")\n",
    "            report.append(\"\")\n",
    "        for scenario_name, res in stress_results.items():\n",
    "            scenario_info = res.get(\"scenario\", {})\n",
    "            metrics = res.get(\"portfolio_metrics\", {})\n",
    "            reg = res.get(\"regime_stats\", {})\n",
    "            \n",
    "            report.append(f\"### {scenario_info.get('name', scenario_name)}\")\n",
    "            report.append(\"\")\n",
    "            report.append(f\"**{scenario_info.get('description', '')}**\")\n",
    "            report.append(\"\")\n",
    "            \n",
    "            # Agregar justificaci√≥n econ√≥mica\n",
    "            if scenario_name in scenario_justifications:\n",
    "                report.append(scenario_justifications[scenario_name])\n",
    "                report.append(\"\")\n",
    "            \n",
    "            # M√©tricas clave solo (especificando horizonte diario)\n",
    "            var_99 = metrics.get(\"VaR 99%\", float(\"nan\"))\n",
    "            cvar_99 = metrics.get(\"CVaR 99%\", float(\"nan\"))\n",
    "            vol_ann = metrics.get(\"Volatility (ann)\", float(\"nan\"))\n",
    "            \n",
    "            report.append(f\"- **VaR 99% (diario):** {var_99:.4f} | **CVaR 99% (diario):** {cvar_99:.4f} | **Volatilidad anualizada:** {vol_ann:.4f}\")\n",
    "            report.append(\"\")\n",
    "            \n",
    "            # Reg√≠menes simulados (solo si hay datos)\n",
    "            if real_regime_summary is not None and reg:\n",
    "                pct_crisis_sim = reg.get(\"%_state1\", float(\"nan\"))\n",
    "                pct_crisis_real = real_regime_summary.get(\"%_state1\", float(\"nan\"))\n",
    "                report.append(\n",
    "                    f\"- **Tiempo en crisis:** {pct_crisis_sim:.1f}% (vs {pct_crisis_real:.1f}% hist√≥rico). \"\n",
    "                    \"El escenario fuerza condiciones adversas mediante matriz de transici√≥n modificada.\"\n",
    "                )\n",
    "                report.append(\"\")\n",
    "        \n",
    "        report.append(\"\")\n",
    "        report.append(\"![Comparaci√≥n de VaR/CVaR 99%](../figures/phase5_scenario_risk.png)\")\n",
    "        report.append(\"\")\n",
    "\n",
    "        report.append(\n",
    "            f\"**Recomendaci√≥n al Comit√©:** Los escenarios muestran que bajo condiciones de estr√©s \"\n",
    "            f\"persistente, las p√©rdidas extremas diarias (CVaR 99% diario) pueden alcanzar {min_cvar:.1f}% a {max_cvar:.1f}%, \"\n",
    "            f\"con volatilidades anualizadas del {min_vol:.0f}-{max_vol:.0f}%. La diversificaci√≥n desaparece cuando las \"\n",
    "            \"correlaciones convergen hacia 1 en crisis, amplificando el riesgo de cola de la cartera.\"\n",
    "        )\n",
    "        report.append(\"\")\n",
    "    else:\n",
    "        report.append(\"_Escenarios pendientes de ejecuci√≥n._\")\n",
    "        report.append(\"\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # CONCLUSIONES\n",
    "    # ============================================================================\n",
    "    # ============================================================================\n",
    "    # RECOMENDACIONES ESTRAT√âGICAS\n",
    "    # ============================================================================\n",
    "    report.append(\"---\")\n",
    "    report.append(\"\")\n",
    "    report.append(\"## Recomendaciones Estrat√©gicas\")\n",
    "    report.append(\"\")\n",
    "    report.append(\n",
    "        \"Basado en el an√°lisis de reg√≠menes y escenarios de estr√©s, se presentan las siguientes \"\n",
    "        \"recomendaciones estrat√©gicas para la gesti√≥n de riesgo de la cartera:\"\n",
    "    )\n",
    "    report.append(\"\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    # Calcular duraciones medias din√°micamente\n",
    "    mean_dur_calm = real_regime_summary.get(\"mean_duration_state0\", 24) if real_regime_summary else 24\n",
    "    mean_dur_crisis = real_regime_summary.get(\"mean_duration_state1\", 35) if real_regime_summary else 35\n",
    "    min_dur = min(mean_dur_calm, mean_dur_crisis)\n",
    "    max_dur = max(mean_dur_calm, mean_dur_crisis)\n",
    "    \n",
    "    report.append(\"### 1. Monitoreo Activo de Reg√≠menes\")\n",
    "    report.append(\"\")\n",
    "    report.append(\n",
    "        \"**Acci√≥n:** Implementar sistema de alerta temprana basado en detecci√≥n de transici√≥n \"\n",
    "        f\"a r√©gimen de CRISIS. Las duraciones medias de {min_dur:.0f}-{max_dur:.0f} d√≠as permiten ajustes proactivos \"\n",
    "        \"de exposici√≥n antes de que el riesgo se materialice completamente.\"\n",
    "    )\n",
    "    report.append(\"\")\n",
    "    report.append(\"### 2. Gesti√≥n de Diversificaci√≥n en Crisis\")\n",
    "    report.append(\"\")\n",
    "    report.append(\n",
    "        f\"**Acci√≥n:** Reducir exposici√≥n a activos pro-c√≠clicos (especialmente High Yield) \"\n",
    "        f\"cuando se detecte transici√≥n a CRISIS. El aumento de correlaciones ({corr_change_avg*100:.0f}) elimina \"\n",
    "        \"beneficios de diversificaci√≥n, requiriendo reducci√≥n de tama√±o de posici√≥n o cobertura.El High Yield es el activo m√°s pro-c√≠clico \"\n",
    "        f\"(volatilidad +{hyg_vol_increase:.0f}% en crisis).\"\n",
    "    )\n",
    "    report.append(\"\")\n",
    "    \n",
    "    # Texto adaptativo para stress testing\n",
    "    if stress_results and len(stress_results) > 0:\n",
    "        stress_text = (\n",
    "            f\"Los escenarios de estr√©s cuantifican p√©rdidas extremas coherentes \"\n",
    "            f\"con crisis hist√≥ricas. Bajo condiciones adversas, el CVaR 99% (diario) alcanza \"\n",
    "            f\"{min_cvar:.1f}% a {max_cvar:.1f}%, con volatilidades anualizadas del {min_vol:.0f}-{max_vol:.0f}%. \"\n",
    "            \"El motor permite \\\"romper la cartera\\\" mediante condiciones \"\n",
    "            \"econ√≥micamente justificadas, proporcionando m√©tricas de riesgo interpretables para \"\n",
    "            \"el Comit√© de Riesgos.\"\n",
    "        )\n",
    "    else:\n",
    "        stress_text = (\n",
    "            \"Los escenarios de estr√©s cuantifican p√©rdidas extremas coherentes \"\n",
    "            \"con crisis hist√≥ricas. El motor permite \\\"romper la cartera\\\" mediante condiciones \"\n",
    "            \"econ√≥micamente justificadas, proporcionando m√©tricas de riesgo interpretables para \"\n",
    "            \"el Comit√© de Riesgos.\"\n",
    "        )\n",
    "    \n",
    "    report.append(\"### 3. Stress Testing Continuo\")\n",
    "    # Calcular aumentos de CVaR para recomendaci√≥n 3\n",
    "    cvar_increase_min = 0\n",
    "    cvar_increase_max = 0\n",
    "    if stress_results is not None and phase4_results is not None:\n",
    "        base_metrics_temp = phase4_results.get(\"simulated_portfolio\", {}) if phase4_results else {}\n",
    "        base_cvar_temp = base_metrics_temp.get(\"CVaR 99%\", float(\"nan\"))\n",
    "        if not np.isnan(base_cvar_temp) and base_cvar_temp != 0:\n",
    "            increases = []\n",
    "            for scenario_name, res in stress_results.items():\n",
    "                metrics = res.get(\"portfolio_metrics\", {})\n",
    "                cvar = metrics.get(\"CVaR 99%\", float(\"nan\"))\n",
    "                if not np.isnan(cvar):\n",
    "                    inc = ((cvar - base_cvar_temp) / abs(base_cvar_temp)) * 100\n",
    "                    increases.append(inc)\n",
    "            if increases:\n",
    "                cvar_increase_min = min(increases)\n",
    "                cvar_increase_max = max(increases)\n",
    "    \n",
    "    report.append(\"\")\n",
    "    report.append(\n",
    "        f\"**Acci√≥n:** Ejecutar escenarios de estr√©s trimestralmente y actualizar m√©tricas de \"\n",
    "        f\"CVaR. Los escenarios muestran aumentos del CVaR 99% (diario) de {cvar_increase_min:.1f}% a {cvar_increase_max:.1f}% respecto a base, \"\n",
    "        \"requiriendo ajuste de l√≠mites de riesgo y capital econ√≥mico.\"\n",
    "    )\n",
    "    report.append(\"\")\n",
    "    report.append(\"\")\n",
    "\n",
    "    # Write to disk\n",
    "    report_text = \"\\n\".join(report)\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(report_text)\n",
    "\n",
    "    return report_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d304d7",
   "metadata": {},
   "source": [
    "### Main ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "ee8769f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Fases 1 - Detectando el Pulso del Mercado (Hidden Markov Models)\n",
      "================================================================================\n",
      "MARKET VARIABLES USED FOR REGIME DETECTION (Multivariate Gaussian HMM):\n",
      "================================================================================\n",
      "1. ^GSPC\n",
      "2. ^VIX\n",
      "3. GS10\n",
      "4. GS2\n",
      "5. yield_slope\n",
      "6. BAMLH0A0HYM2\n",
      "\n",
      "Total dimensions: 6 variables √ó 5387 observations\n",
      "================================================================================\n",
      "\n",
      "FEATURE ANALYSIS - Mean and Volatility per Regime:\n",
      "================================================================================\n",
      "    Variable Regime  Mean (HMM)  Std Dev (HMM)\n",
      "BAMLH0A0HYM2   CALM   -0.075614       0.743981\n",
      "BAMLH0A0HYM2 CRISIS    0.108246       1.272512\n",
      "        GS10   CALM    0.062325       0.497263\n",
      "        GS10 CRISIS   -0.089222       1.436685\n",
      "         GS2   CALM    0.031689       0.408406\n",
      "         GS2 CRISIS   -0.045365       1.479626\n",
      "       ^GSPC   CALM    0.053335       0.561592\n",
      "       ^GSPC CRISIS   -0.076353       1.403628\n",
      "        ^VIX   CALM   -0.070836       0.698778\n",
      "        ^VIX CRISIS    0.101406       1.309612\n",
      " yield_slope   CALM   -0.269208       0.999758\n",
      " yield_slope CRISIS    0.385386       0.865118\n",
      "================================================================================\n",
      "\n",
      "ESTAD√çSTICAS DE LIMPIEZA DE DATOS:\n",
      "================================================================================\n",
      "  Observaciones originales: 5387\n",
      "  Observaciones despu√©s de limpieza: 4981\n",
      "  Observaciones removidas (NaN/Inf): 406 (7.54%)\n",
      "  Variables analizadas: 6\n",
      "================================================================================\n",
      "\n",
      "HMM TRANSITION MATRIX (Probabilidades de Transici√≥n):\n",
      "================================================================================\n",
      "          CRISIS      CALM\n",
      "CRISIS  0.951695  0.048305\n",
      "CALM    0.033918  0.966082\n",
      "\n",
      "Interpretaci√≥n:\n",
      "  - Probabilidad de permanecer en CALMA: 0.9661\n",
      "  - Probabilidad de transici√≥n CALMA ‚Üí CRISIS: 0.0339\n",
      "  - Probabilidad de permanecer en CRISIS: 0.9517\n",
      "  - Probabilidad de transici√≥n CRISIS ‚Üí CALMA: 0.0483\n",
      "================================================================================\n",
      "\n",
      "HMM STATE PARAMETERS (Medias y Covarianzas por Estado):\n",
      "================================================================================\n",
      "\n",
      "Estado 0 (CRISIS):\n",
      "  Volatilidad promedio (Frobenius norm): 1.310619\n",
      "  Vector de medias (mean):\n",
      "    ^GSPC: -0.076353\n",
      "    ^VIX: 0.101406\n",
      "    GS10: -0.089222\n",
      "    GS2: -0.045365\n",
      "    yield_slope: 0.385386\n",
      "    BAMLH0A0HYM2: 0.108246\n",
      "  Varianzas (diagonal de matriz de covarianza):\n",
      "    ^GSPC: 1.970172\n",
      "    ^VIX: 1.715083\n",
      "    GS10: 2.064064\n",
      "    GS2: 2.189294\n",
      "    yield_slope: 0.748429\n",
      "    BAMLH0A0HYM2: 1.619286\n",
      "\n",
      "Estado 1 (CALM):\n",
      "  Volatilidad promedio (Frobenius norm): 0.679555\n",
      "  Vector de medias (mean):\n",
      "    ^GSPC: 0.053335\n",
      "    ^VIX: -0.070836\n",
      "    GS10: 0.062325\n",
      "    GS2: 0.031689\n",
      "    yield_slope: -0.269208\n",
      "    BAMLH0A0HYM2: -0.075614\n",
      "  Varianzas (diagonal de matriz de covarianza):\n",
      "    ^GSPC: 0.315386\n",
      "    ^VIX: 0.488291\n",
      "    GS10: 0.247270\n",
      "    GS2: 0.166795\n",
      "    yield_slope: 0.999517\n",
      "    BAMLH0A0HYM2: 0.553508\n",
      "================================================================================\n",
      "\n",
      "Fase 1 - Executition time: 0:00:18.187360\n",
      "Running Fases 2 - Anatom√≠a del Riesgo (An√°lisis Marginal)\n",
      "================================================================================\n",
      "FASE 1: AN√ÅLISIS DE RIESGO INDIVIDUAL POR R√âGIMEN\n",
      "================================================================================\n",
      "\n",
      "1.1 Separando datos por r√©gimen...\n",
      "     ‚úì D√≠as en CALMA: 2951\n",
      "     ‚úì D√≠as en CRISIS: 2030\n",
      "\n",
      "1.2 Calculando estad√≠sticas marginales...\n",
      "     ‚úì 36 filas de estad√≠sticas (activos √ó reg√≠menes)\n",
      "\n",
      "1.3 Analizando activos clave (HYG, GLD)...\n",
      "     ‚úì An√°lisis detallado de 2 activos\n",
      "\n",
      "1.4 Comparando volatilidades entre reg√≠menes...\n",
      "     ‚úì Tabla de comparaci√≥n creada\n",
      "\n",
      "1.4 Generando interpretaci√≥n econ√≥mica...\n",
      "================================================================================\n",
      "INTERPRETACI√ìN ECON√ìMICA DE CAMBIOS DE R√âGIMEN\n",
      "================================================================================\n",
      "\n",
      "üìä HIGH YIELD (HYG) - Bonos de Alto Rendimiento\n",
      "--------------------------------------------------------------------------------\n",
      "  ‚Ä¢ Volatilidad en CALMA: 0.35%\n",
      "  ‚Ä¢ Volatilidad en CRISIS: 0.98%\n",
      "  ‚Ä¢ Aumento: 180.7%\n",
      "\n",
      "  INTERPRETACI√ìN:\n",
      "  El aumento de volatilidad en crisis refleja:\n",
      "  ‚úì Mayor aversi√≥n al riesgo en el mercado\n",
      "  ‚úì Widening de spreads de cr√©dito\n",
      "  ‚úì Stress en el segmento de bonos de alto rendimiento\n",
      "  ‚Üí El HYG es PRO-C√çCLICO (amplifica riesgo en crisis)\n",
      "\n",
      "üèÜ ORO (GLD) - Activo Refugio\n",
      "--------------------------------------------------------------------------------\n",
      "  ‚Ä¢ Retorno medio en CALMA: 0.04%\n",
      "  ‚Ä¢ Retorno medio en CRISIS: 0.04%\n",
      "  ‚Ä¢ Volatilidad en CALMA: 1.01%\n",
      "  ‚Ä¢ Volatilidad en CRISIS: 1.31%\n",
      "\n",
      "  INTERPRETACI√ìN:\n",
      "  ‚ö† El ORO NO act√∫a como refugio esperado\n",
      "  ‚ö† Posible liquidaci√≥n forzada en crisis\n",
      "  ‚Üí Revisar correlaci√≥n con equity en stress\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Guardando resultados de Fase 1...\n",
      "     ‚úì Resultados guardados en C:\\Users\\piett\\OneDrive\\Desktop\\Pietro\\Master MIAX\\Clases\\2.Introduccion a los Sistemas Financieros\\Tareas\\Riesgos\\data\\gold\n",
      "\n",
      "Fase 2 - Executition time: 0:00:00.824949\n",
      "Running Fase 3 ‚Äì C√≥pulas y correlaciones por r√©gimen\n",
      "Correlation change (CRISIS - CALM):\n",
      "  Mean off-diagonal change: 0.171\n",
      "  Max off-diagonal increase: 0.451\n",
      "PAR√ÅMETROS DE C√ìPULAS (Matrices de Correlaci√≥n por R√©gimen):\n",
      "================================================================================\n",
      "\n",
      "R√©gimen: CALM\n",
      "--------------------------------------------------------------------------------\n",
      "Matriz de Correlaci√≥n (C√≥pula Gaussiana):\n",
      "           AAPL      AMZN       BAC     BRK-B       CVX      ENPH       GLD       GME     GOOGL       HYG\n",
      "AAPL   1.000000  0.291462  0.162111  0.203914  0.132087  0.185402  0.035739  0.120548  0.355302  0.291472\n",
      "AMZN   0.291462  1.000000  0.197404  0.216409  0.111621  0.171544  0.026700  0.122894  0.434109  0.292892\n",
      "BAC    0.162111  0.197404  1.000000  0.439810  0.266276  0.147757 -0.021424  0.116254  0.220368  0.296303\n",
      "BRK-B  0.203914  0.216409  0.439810  1.000000  0.325342  0.149438 -0.030589  0.087184  0.213950  0.334025\n",
      "CVX    0.132087  0.111621  0.266276  0.325342  1.000000  0.115101  0.170634  0.094138  0.139774  0.289377\n",
      "ENPH   0.185402  0.171544  0.147757  0.149438  0.115101  1.000000  0.053950  0.095796  0.136443  0.215932\n",
      "GLD    0.035739  0.026700 -0.021424 -0.030589  0.170634  0.053950  1.000000  0.021676  0.056763  0.167642\n",
      "GME    0.120548  0.122894  0.116254  0.087184  0.094138  0.095796  0.021676  1.000000  0.112618  0.145355\n",
      "GOOGL  0.355302  0.434109  0.220368  0.213950  0.139774  0.136443  0.056763  0.112618  1.000000  0.299797\n",
      "HYG    0.291472  0.292892  0.296303  0.334025  0.289377  0.215932  0.167642  0.145355  0.299797  1.000000\n",
      "\n",
      "... (matriz completa: 18x18)\n",
      "\n",
      "Estad√≠sticas de correlaci√≥n (fuera de diagonal):\n",
      "  Media: 0.1498\n",
      "  M√≠nimo: -0.0338\n",
      "  M√°ximo: 0.7871\n",
      "  Desviaci√≥n est√°ndar: 0.1410\n",
      "\n",
      "\n",
      "R√©gimen: CRISIS\n",
      "--------------------------------------------------------------------------------\n",
      "Matriz de Correlaci√≥n (C√≥pula Gaussiana):\n",
      "           AAPL      AMZN       BAC     BRK-B       CVX      ENPH       GLD       GME     GOOGL       HYG\n",
      "AAPL   1.000000  0.600153  0.475245  0.486822  0.489215  0.339218 -0.013708  0.203136  0.681569  0.501764\n",
      "AMZN   0.600153  1.000000  0.388735  0.407550  0.402020  0.259159 -0.015277  0.170076  0.636010  0.411821\n",
      "BAC    0.475245  0.388735  1.000000  0.612696  0.542069  0.295558 -0.086322  0.179190  0.465659  0.518032\n",
      "BRK-B  0.486822  0.407550  0.612696  1.000000  0.597623  0.270495 -0.013485  0.174859  0.531280  0.543983\n",
      "CVX    0.489215  0.402020  0.542069  0.597623  1.000000  0.301498  0.114122  0.183754  0.521414  0.564292\n",
      "ENPH   0.339218  0.259159  0.295558  0.270495  0.301498  1.000000  0.089518  0.081418  0.295273  0.387260\n",
      "GLD   -0.013708 -0.015277 -0.086322 -0.013485  0.114122  0.089518  1.000000 -0.016839 -0.007403 -0.001840\n",
      "GME    0.203136  0.170076  0.179190  0.174859  0.183754  0.081418 -0.016839  1.000000  0.178101  0.179521\n",
      "GOOGL  0.681569  0.636010  0.465659  0.531280  0.521414  0.295273 -0.007403  0.178101  1.000000  0.520950\n",
      "HYG    0.501764  0.411821  0.518032  0.543983  0.564292  0.387260 -0.001840  0.179521  0.520950  1.000000\n",
      "\n",
      "... (matriz completa: 18x18)\n",
      "\n",
      "Estad√≠sticas de correlaci√≥n (fuera de diagonal):\n",
      "  Media: 0.3212\n",
      "  M√≠nimo: -0.0863\n",
      "  M√°ximo: 0.8814\n",
      "  Desviaci√≥n est√°ndar: 0.2288\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Fase 3 - Executition time: 0:00:03.309669\n",
      "Running Fase 4 ‚Äì Simulador Monte Carlo\n",
      "Fase 4 - Executition time: 0:00:01.657585\n",
      "RESULTADOS NUM√âRICOS DEL MOTOR DE SIMULACI√ìN:\n",
      "================================================================================\n",
      "\n",
      "1. M√âTRICAS DE RIESGO (Cartera Equiponderada):\n",
      "--------------------------------------------------------------------------------\n",
      "| M√©trica | Real (hist√≥rico) | Simulado (Monte Carlo) |\n",
      "|---------|-------------------|-------------------------|\n",
      "| Volatility (ann) | 0.169581 | 0.231143 |\n",
      "| Max Drawdown | -0.303872 | -0.430863 |\n",
      "| VaR 99% | -0.026980 | -0.036406 |\n",
      "| CVaR 99% | -0.039184 | -0.043147 |\n",
      "\n",
      "2. REPRODUCCI√ìN DE REG√çMENES:\n",
      "--------------------------------------------------------------------------------\n",
      "| Estad√≠stico | Real | Simulado |\n",
      "|-------------|------|----------|\n",
      "| % de d√≠as en estado calma | 40.75 | 35.50 |\n",
      "| % de d√≠as en estado crisis | 59.25 | 64.50 |\n",
      "| Duraci√≥n media estado calma | 24.17 | 17.75 |\n",
      "| Duraci√≥n media estado crisis | 35.13 | 25.64 |\n",
      "| N√∫mero de cambios de estado | 167.00 | 4.69 |\n",
      "================================================================================\n",
      "\n",
      "Running Fase 5 ‚Äì Escenarios de estr√©s\n",
      "Fase 5 - Executition time: 0:00:07.836854\n",
      "RESULTADOS NUM√âRICOS DE ESCENARIOS DE ESTR√âS:\n",
      "================================================================================\n",
      "\n",
      "Escenario: Stagflation 2022\n",
      "--------------------------------------------------------------------------------\n",
      "M√©tricas de Riesgo:\n",
      "  Volatility (ann): 0.197601\n",
      "  Max Drawdown: -0.297887\n",
      "  VaR 99%: -0.032740\n",
      "  CVaR 99%: -0.039786\n",
      "\n",
      "Estad√≠sticas de R√©gimen Simulado:\n",
      "  % d√≠as en calma: 63.93%\n",
      "  % d√≠as en crisis: 36.07%\n",
      "  Duraci√≥n media calma: 17.94 d√≠as\n",
      "  Duraci√≥n media crisis: 9.51 d√≠as\n",
      "  N√∫mero de cambios: 8\n",
      "\n",
      "\n",
      "Escenario: Credit Crisis 2008\n",
      "--------------------------------------------------------------------------------\n",
      "M√©tricas de Riesgo:\n",
      "  Volatility (ann): 0.197648\n",
      "  Max Drawdown: -0.327832\n",
      "  VaR 99%: -0.033389\n",
      "  CVaR 99%: -0.042122\n",
      "\n",
      "Estad√≠sticas de R√©gimen Simulado:\n",
      "  % d√≠as en calma: 77.98%\n",
      "  % d√≠as en crisis: 22.02%\n",
      "  Duraci√≥n media calma: 17.74 d√≠as\n",
      "  Duraci√≥n media crisis: 4.86 d√≠as\n",
      "  N√∫mero de cambios: 10\n",
      "\n",
      "\n",
      "Escenario: Mixed Shock\n",
      "--------------------------------------------------------------------------------\n",
      "M√©tricas de Riesgo:\n",
      "  Volatility (ann): 0.183239\n",
      "  Max Drawdown: -0.446135\n",
      "  VaR 99%: -0.030584\n",
      "  CVaR 99%: -0.038131\n",
      "\n",
      "Estad√≠sticas de R√©gimen Simulado:\n",
      "  % d√≠as en calma: 73.02%\n",
      "  % d√≠as en crisis: 26.98%\n",
      "  Duraci√≥n media calma: 17.80 d√≠as\n",
      "  Duraci√≥n media crisis: 6.31 d√≠as\n",
      "  N√∫mero de cambios: 10\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úì PDF generado: C:\\Users\\piett\\OneDrive\\Desktop\\Pietro\\Master MIAX\\Clases\\2.Introduccion a los Sistemas Financieros\\Tareas\\Riesgos\\report\\INFORME_EJECUTIVO.pdf\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Fases 0 - Datos\n",
    "    ensure_directories()\n",
    "    set_global_seed()\n",
    "\n",
    "    portfolio_instance = portfolio()\n",
    "    market_data_df = market_risk()\n",
    "\n",
    "    # Fases 1 - Detectando el \"Pulso\" del Mercado (Hidden Markov Models)\n",
    "    print(\"Running Fases 1 - Detectando el Pulso del Mercado (Hidden Markov Models)\")\n",
    "    start_time = datetime.now()\n",
    "    regime_stats, regimes, hmm_results, log_returns_clean, sp500_prices_clean = run_regime_detection_pipeline()\n",
    "    \n",
    "    # REQUISITO T√âCNICO: Exibir estad√≠sticas de limpieza de datos y par√°metros de HMM\n",
    "    log_returns_original, _ = load_and_prepare_returns(COMBINED_PATH)\n",
    "    display_data_cleaning_stats(log_returns_original, log_returns_clean)\n",
    "    display_hmm_parameters(hmm_results, log_returns_clean)\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    exec_time = end_time - start_time\n",
    "    print(f\"Fase 1 - Executition time: {exec_time}\")\n",
    "\n",
    "    # Fases 2 - Anatom√≠a del Riesgo (An√°lisis Marginal)\n",
    "    print(\"Running Fases 2 - Anatom√≠a del Riesgo (An√°lisis Marginal)\")\n",
    "    start_time = datetime.now()\n",
    "    df_stats, df_key_assets, df_vol_comparison, interpretation = run_phase1_risk_analysis(\n",
    "        portfolio_instance,\n",
    "        regimes,\n",
    "        log_returns_clean,\n",
    "        hmm_results,\n",
    "    )\n",
    "    end_time = datetime.now()\n",
    "    exec_time = end_time - start_time\n",
    "    print(f\"Fase 2 - Executition time: {exec_time}\")\n",
    "\n",
    "    # Fase 3 ‚Äì C√≥pulas y correlaciones por r√©gimen\n",
    "    print(\"Running Fase 3 ‚Äì C√≥pulas y correlaciones por r√©gimen\")\n",
    "    start_time = datetime.now()\n",
    "    corr_by_regime, copulas_by_regime = run_phase3_copula_analysis(\n",
    "        portfolio_instance,\n",
    "        regimes,\n",
    "        log_returns_clean,\n",
    "        hmm_results,\n",
    "    )\n",
    "    \n",
    "    # REQUISITO T√âCNICO: Exibir par√°metros de c√≥pulas (matrices de correlaci√≥n)\n",
    "    display_copula_parameters(corr_by_regime, copulas_by_regime)\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    exec_time = end_time - start_time\n",
    "    print(f\"Fase 3 - Executition time: {exec_time}\")\n",
    "\n",
    "     # Fase 4 ‚Äì Simulador Monte Carlo\n",
    "    print(\"Running Fase 4 ‚Äì Simulador Monte Carlo\")\n",
    "    start_time = datetime.now()\n",
    "    phase4_results = run_phase4_simulation(\n",
    "        portfolio=portfolio_instance,\n",
    "        hmm_results=hmm_results,\n",
    "        df_stats=df_stats,\n",
    "        corr_by_regime=corr_by_regime,\n",
    "        copulas_by_regime=copulas_by_regime,\n",
    "        n_paths=100,\n",
    "        n_days=126,\n",
    "    )\n",
    "    end_time = datetime.now()\n",
    "    exec_time = end_time - start_time\n",
    "    print(f\"Fase 4 - Executition time: {exec_time}\")\n",
    "\n",
    "    real_regime_summary = summarize_regime_paths(regimes.reshape(1, -1))\n",
    "    \n",
    "    # REQUISITO T√âCNICO: Exibir resultados num√©ricos del motor de simulaci√≥n\n",
    "    display_simulation_results(phase4_results, real_regime_summary)\n",
    "\n",
    "    # Fase 5 ‚Äì Escenarios de estr√©s\n",
    "    print(\"Running Fase 5 ‚Äì Escenarios de estr√©s\")\n",
    "    start_time = datetime.now()\n",
    "    scenarios = build_default_stress_scenarios(hmm_results)\n",
    "    stress_results: Dict[str, Dict[str, Dict[str, float]]] = {}\n",
    "\n",
    "    for scenario in scenarios:\n",
    "        stress_results[scenario.name] = run_stress_scenario(\n",
    "            portfolio=portfolio_instance,\n",
    "            hmm_results=hmm_results,\n",
    "            df_stats=df_stats,\n",
    "            copulas_by_regime=copulas_by_regime,\n",
    "            scenario=scenario,\n",
    "            n_paths=100,\n",
    "            n_days=126,\n",
    "        )\n",
    "    end_time = datetime.now()\n",
    "    exec_time = end_time - start_time\n",
    "    print(f\"Fase 5 - Executition time: {exec_time}\")\n",
    "\n",
    "    # Fase 5 ‚Äì gr√°fico comparativo VaR/CVaR por escenario\n",
    "    plot_phase5_scenario_risk(stress_results, output_dir=FIGURES_DIR)\n",
    "    \n",
    "    # REQUISITO T√âCNICO: Exibir resultados num√©ricos de escenarios de estr√©s\n",
    "    display_stress_scenario_results(stress_results)\n",
    "\n",
    "    # Generar informe ejecutivo markdown (versi√≥n concisa para Comit√© de Riesgos)\n",
    "    markdown_path = REPORT_DIR / \"INFORME_EJECUTIVO.md\"\n",
    "    generate_executive_report(\n",
    "        output_path=markdown_path,\n",
    "        regime_stats=regime_stats,\n",
    "        df_stats=df_stats,\n",
    "        df_key_assets=df_key_assets,\n",
    "        df_vol_comparison=df_vol_comparison,\n",
    "        real_regime_summary=real_regime_summary,\n",
    "        phase4_results=phase4_results,\n",
    "        stress_results=stress_results,\n",
    "        corr_by_regime=corr_by_regime,\n",
    "    )\n",
    "    \n",
    "    # Generar PDF a partir del markdown\n",
    "    pdf_path = REPORT_DIR / \"INFORME_EJECUTIVO.pdf\"\n",
    "    markdown_to_pdf(markdown_path, pdf_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
